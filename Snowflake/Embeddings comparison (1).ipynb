{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, lit, call_function\n",
    "from snowflake.snowpark.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from openai import OpenAI\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"  # ‚ö†Ô∏è Replace with your actual OpenAI API key\n",
    "SAMPLE_SIZE = 10000  # Fixed at 10,000 rows\n",
    "\n",
    "# Initialize OpenAI client\n",
    "if OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY_HERE\":\n",
    "    print(\"‚ùå ERROR: Please update OPENAI_API_KEY with your actual key!\")\n",
    "    print(\"   Get your key from: https://platform.openai.com/api-keys\")\n",
    "    client = None\n",
    "else:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"‚úÖ OpenAI client initialized\")\n",
    "    print(f\"üìä Sample size: {SAMPLE_SIZE:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Test session\n",
    "print(\"‚úÖ Snowflake session active\")\n",
    "print(f\"   Current role: {session.get_current_role()}\")\n",
    "print(f\"   Current database: {session.get_current_database()}\")\n",
    "print(f\"   Current warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60b8e9-8a65-4272-8f29-6fb43587af8e",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "source_table = \"MEALMINDDATA.DBT_VAISHNAVIPATEL15_MARTS.MART_MEAL_PLANNING_CLEANED\"\n",
    "embeddings_schema = \"MEALMINDDATA.EMBEDDINGS_COMPARISON\"\n",
    "\n",
    "# Create schema\n",
    "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {embeddings_schema}\").collect()\n",
    "print(f\"‚úÖ Schema {embeddings_schema} ready\")\n",
    "\n",
    "# Check source table\n",
    "row_count = session.sql(f\"SELECT COUNT(*) as cnt FROM {source_table}\").collect()[0]['CNT']\n",
    "print(f\"üìä Source table has {row_count:,} total rows\")\n",
    "print(f\"üìå We will process {SAMPLE_SIZE:,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c98b7-0b9a-4ca4-bf2a-a549bf008d4f",
   "metadata": {
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": [
    "source_table = \"MEALMINDDATA.DBT_VAISHNAVIPATEL15_MARTS.MART_MEAL_PLANNING_CLEANED\"\n",
    "embeddings_schema = \"MEALMINDDATA.EMBEDDINGS_COMPARISON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd1fb3-03e7-405d-a743-a5eafc1a8ce1",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {embeddings_schema}.SNOWFLAKE_EMBEDDINGS (\n",
    "        FDC_ID NUMBER(38,0),\n",
    "        FOOD_NAME VARCHAR(1000),\n",
    "        CATEGORY VARCHAR(1000),\n",
    "        EMBEDDING_TEXT VARCHAR,\n",
    "        EMBEDDING VECTOR(FLOAT, 768),\n",
    "        GENERATION_TIME_MS FLOAT,\n",
    "        MODEL_NAME VARCHAR DEFAULT 'snowflake-arctic-embed-m',\n",
    "        CREATED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"‚úÖ Created SNOWFLAKE_EMBEDDINGS table\")\n",
    "\n",
    "# OpenAI embeddings table\n",
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {embeddings_schema}.OPENAI_EMBEDDINGS (\n",
    "        FDC_ID NUMBER(38,0),\n",
    "        FOOD_NAME VARCHAR(1000),\n",
    "        CATEGORY VARCHAR(1000),\n",
    "        EMBEDDING_TEXT VARCHAR,\n",
    "        EMBEDDING VECTOR(FLOAT, 1536),\n",
    "        GENERATION_TIME_MS FLOAT,\n",
    "        MODEL_NAME VARCHAR DEFAULT 'text-embedding-3-small',\n",
    "        CREATED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"‚úÖ Created OPENAI_EMBEDDINGS table\")\n",
    "\n",
    "# Metrics comparison table\n",
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {embeddings_schema}.COMPARISON_METRICS (\n",
    "        METRIC_TYPE VARCHAR,\n",
    "        METRIC_NAME VARCHAR,\n",
    "        SNOWFLAKE_VALUE VARCHAR,\n",
    "        OPENAI_VALUE VARCHAR,\n",
    "        COMPARISON VARCHAR,\n",
    "        RUN_DATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"‚úÖ Created COMPARISON_METRICS table\")\n",
    "print(\"\\nüìÅ All tables created in schema:\", embeddings_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b2598-980a-43bd-ae3c-240cf2dc6227",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {embeddings_schema}.OPENAI_EMBEDDINGS (\n",
    "        FDC_ID NUMBER(38,0),\n",
    "        FOOD_NAME VARCHAR(1000),\n",
    "        CATEGORY VARCHAR(1000),\n",
    "        EMBEDDING_TEXT VARCHAR,\n",
    "        EMBEDDING VECTOR(FLOAT, 1536),\n",
    "        GENERATION_TIME_MS FLOAT,\n",
    "        MODEL_NAME VARCHAR DEFAULT 'text-embedding-3-small',\n",
    "        CREATED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32695831-7bf2-431f-8da4-4339e789a698",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            FDC_ID,\n",
    "            FOOD_NAME,\n",
    "            CATEGORY,\n",
    "            CALORIES,\n",
    "            PROTEIN,\n",
    "            CARBOHYDRATE,\n",
    "            TOTAL_FAT,\n",
    "            FIBER,\n",
    "            SODIUM,\n",
    "            CONCAT(\n",
    "                'Food: ', FOOD_NAME, \n",
    "                ' Category: ', COALESCE(CATEGORY, 'Unknown'),\n",
    "                ' Calories: ', COALESCE(CALORIES::VARCHAR, 'N/A'),\n",
    "                ' Protein: ', COALESCE(PROTEIN::VARCHAR, 'N/A'), 'g',\n",
    "                ' Carbs: ', COALESCE(CARBOHYDRATE::VARCHAR, 'N/A'), 'g',\n",
    "                ' Fat: ', COALESCE(TOTAL_FAT::VARCHAR, 'N/A'), 'g',\n",
    "                ' Fiber: ', COALESCE(FIBER::VARCHAR, 'N/A'), 'g',\n",
    "                ' Sodium: ', COALESCE(SODIUM::VARCHAR, 'N/A'), 'mg'\n",
    "            ) AS EMBEDDING_TEXT\n",
    "        FROM {source_table}\n",
    "        WHERE FOOD_NAME IS NOT NULL\n",
    "        ORDER BY FDC_ID\n",
    "        LIMIT {SAMPLE_SIZE}\n",
    "    \"\"\"\n",
    "    return session.sql(query)\n",
    "\n",
    "# Prepare data\n",
    "data_df = prepare_data()\n",
    "print(f\"‚úÖ Prepared {SAMPLE_SIZE:,} rows for embedding\")\n",
    "\n",
    "# Show sample\n",
    "sample = data_df.limit(3).to_pandas()\n",
    "print(\"\\nüìã Sample data:\")\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"\\nFood: {row['FOOD_NAME']}\")\n",
    "    print(f\"Text: {row['EMBEDDING_TEXT'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb31c7-baca-45cd-863e-6e5c42f9e1a0",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "print(\"üîÑ Generating Snowflake Arctic embeddings...\")\n",
    "print(\"   Processing first 5,000 rows...\")\n",
    "\n",
    "# Clear existing data\n",
    "session.sql(f\"TRUNCATE TABLE {embeddings_schema}.SNOWFLAKE_EMBEDDINGS\").collect()\n",
    "\n",
    "start_time = time.time()\n",
    "batch_size = 1000\n",
    "\n",
    "# Process first 5000 rows\n",
    "for i in range(0, 5000, batch_size):\n",
    "    print(f\"   Batch {i//batch_size + 1}/5: rows {i+1}-{i+batch_size}\")\n",
    "    \n",
    "    session.sql(f\"\"\"\n",
    "        INSERT INTO {embeddings_schema}.SNOWFLAKE_EMBEDDINGS\n",
    "        SELECT \n",
    "            FDC_ID,\n",
    "            FOOD_NAME,\n",
    "            CATEGORY,\n",
    "            EMBEDDING_TEXT,\n",
    "            SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', EMBEDDING_TEXT) as EMBEDDING,\n",
    "            {(time.time() - start_time) * 1000 / (i + batch_size)} as GENERATION_TIME_MS,\n",
    "            'snowflake-arctic-embed-m' as MODEL_NAME,\n",
    "            CURRENT_TIMESTAMP() as CREATED_AT\n",
    "        FROM (\n",
    "            SELECT * FROM (\n",
    "                SELECT \n",
    "                    FDC_ID,\n",
    "                    FOOD_NAME,\n",
    "                    CATEGORY,\n",
    "                    CONCAT(\n",
    "                        'Food: ', FOOD_NAME, \n",
    "                        ' Category: ', COALESCE(CATEGORY, 'Unknown'),\n",
    "                        ' Calories: ', COALESCE(CALORIES::VARCHAR, 'N/A'),\n",
    "                        ' Protein: ', COALESCE(PROTEIN::VARCHAR, 'N/A'), 'g',\n",
    "                        ' Carbs: ', COALESCE(CARBOHYDRATE::VARCHAR, 'N/A'), 'g',\n",
    "                        ' Fat: ', COALESCE(TOTAL_FAT::VARCHAR, 'N/A'), 'g'\n",
    "                    ) AS EMBEDDING_TEXT\n",
    "                FROM {source_table}\n",
    "                WHERE FOOD_NAME IS NOT NULL\n",
    "                ORDER BY FDC_ID\n",
    "                LIMIT {i + batch_size}\n",
    "            )\n",
    "            MINUS\n",
    "            SELECT * FROM (\n",
    "                SELECT \n",
    "                    FDC_ID,\n",
    "                    FOOD_NAME,\n",
    "                    CATEGORY,\n",
    "                    CONCAT(\n",
    "                        'Food: ', FOOD_NAME, \n",
    "                        ' Category: ', COALESCE(CATEGORY, 'Unknown'),\n",
    "                        ' Calories: ', COALESCE(CALORIES::VARCHAR, 'N/A'),\n",
    "                        ' Protein: ', COALESCE(PROTEIN::VARCHAR, 'N/A'), 'g',\n",
    "                        ' Carbs: ', COALESCE(CARBOHYDRATE::VARCHAR, 'N/A'), 'g',\n",
    "                        ' Fat: ', COALESCE(TOTAL_FAT::VARCHAR, 'N/A'), 'g'\n",
    "                    ) AS EMBEDDING_TEXT\n",
    "                FROM {source_table}\n",
    "                WHERE FOOD_NAME IS NOT NULL\n",
    "                ORDER BY FDC_ID\n",
    "                LIMIT {i}\n",
    "            )\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "\n",
    "count = session.table(f\"{embeddings_schema}.SNOWFLAKE_EMBEDDINGS\").count()\n",
    "print(f\"‚úÖ Generated {count:,} embeddings so far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cba1cd-2a93-45dd-9661-6fb6be87c403",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "print(\"   Processing second 5,000 rows...\")\n",
    "\n",
    "# Process second 5000 rows\n",
    "for i in range(5000, 10000, batch_size):\n",
    "    print(f\"   Batch {i//batch_size + 1}/10: rows {i+1}-{i+batch_size}\")\n",
    "    \n",
    "    session.sql(f\"\"\"\n",
    "        INSERT INTO {embeddings_schema}.SNOWFLAKE_EMBEDDINGS\n",
    "        SELECT \n",
    "            FDC_ID,\n",
    "            FOOD_NAME,\n",
    "            CATEGORY,\n",
    "            EMBEDDING_TEXT,\n",
    "            SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', EMBEDDING_TEXT) as EMBEDDING,\n",
    "            {(time.time() - start_time) * 1000 / (i + batch_size)} as GENERATION_TIME_MS,\n",
    "            'snowflake-arctic-embed-m' as MODEL_NAME,\n",
    "            CURRENT_TIMESTAMP() as CREATED_AT\n",
    "        FROM (\n",
    "            SELECT * FROM (\n",
    "                SELECT \n",
    "                    FDC_ID,\n",
    "                    FOOD_NAME,\n",
    "                    CATEGORY,\n",
    "                    CONCAT(\n",
    "                        'Food: ', FOOD_NAME, \n",
    "                        ' Category: ', COALESCE(CATEGORY, 'Unknown'),\n",
    "                        ' Calories: ', COALESCE(CALORIES::VARCHAR, 'N/A'),\n",
    "                        ' Protein: ', COALESCE(PROTEIN::VARCHAR, 'N/A'), 'g',\n",
    "                        ' Carbs: ', COALESCE(CARBOHYDRATE::VARCHAR, 'N/A'), 'g',\n",
    "                        ' Fat: ', COALESCE(TOTAL_FAT::VARCHAR, 'N/A'), 'g'\n",
    "                    ) AS EMBEDDING_TEXT\n",
    "                FROM {source_table}\n",
    "                WHERE FOOD_NAME IS NOT NULL\n",
    "                ORDER BY FDC_ID\n",
    "                LIMIT {i + batch_size}\n",
    "            )\n",
    "            MINUS\n",
    "            SELECT * FROM (\n",
    "                SELECT \n",
    "                    FDC_ID,\n",
    "                    FOOD_NAME,\n",
    "                    CATEGORY,\n",
    "                    CONCAT(\n",
    "                        'Food: ', FOOD_NAME, \n",
    "                        ' Category: ', COALESCE(CATEGORY, 'Unknown'),\n",
    "                        ' Calories: ', COALESCE(CALORIES::VARCHAR, 'N/A'),\n",
    "                        ' Protein: ', COALESCE(PROTEIN::VARCHAR, 'N/A'), 'g',\n",
    "                        ' Carbs: ', COALESCE(CARBOHYDRATE::VARCHAR, 'N/A'), 'g',\n",
    "                        ' Fat: ', COALESCE(TOTAL_FAT::VARCHAR, 'N/A'), 'g'\n",
    "                    ) AS EMBEDDING_TEXT\n",
    "                FROM {source_table}\n",
    "                WHERE FOOD_NAME IS NOT NULL\n",
    "                ORDER BY FDC_ID\n",
    "                LIMIT {i}\n",
    "            )\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "\n",
    "total_time = (time.time() - start_time) * 1000\n",
    "final_count = session.table(f\"{embeddings_schema}.SNOWFLAKE_EMBEDDINGS\").count()\n",
    "\n",
    "print(f\"\\n‚úÖ Snowflake Arctic Embedding Generation Complete!\")\n",
    "print(f\"   Total embeddings: {final_count:,}\")\n",
    "print(f\"   Total time: {total_time/1000:.2f} seconds\")\n",
    "print(f\"   Average time per embedding: {total_time/final_count:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9973f40-51a8-43bb-b8fb-835931e41ba6",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "def get_openai_embeddings_batch(texts: List[str], client: OpenAI) -> Tuple[List[List[float]], int]:\n",
    "    \"\"\"Get embeddings for a batch of texts using OpenAI API\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=texts\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings and maintain order\n",
    "        embeddings = []\n",
    "        for item in response.data:\n",
    "            embeddings.append(item.embedding)\n",
    "        \n",
    "        # Get token usage\n",
    "        total_tokens = response.usage.total_tokens if hasattr(response, 'usage') else len(texts) * 50  # estimate\n",
    "        \n",
    "        return embeddings, total_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "print(\"‚úÖ OpenAI helper function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba19684-b72c-4350-911f-dfc6493646ae",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "print(\"üîÑ Generating OpenAI embeddings...\")\n",
    "print(\"   This will take 15-20 minutes due to API rate limits\")\n",
    "\n",
    "if client is None:\n",
    "    print(\"‚ùå Skipping: OpenAI API key not configured\")\n",
    "else:\n",
    "    # Clear existing data\n",
    "    session.sql(f\"TRUNCATE TABLE {embeddings_schema}.OPENAI_EMBEDDINGS\").collect()\n",
    "    \n",
    "    # Get data as pandas DataFrame\n",
    "    print(\"   Loading data into memory...\")\n",
    "    data_pandas = data_df.to_pandas()\n",
    "    \n",
    "    batch_size = 100  # OpenAI can handle larger batches\n",
    "    total_rows = min(len(data_pandas), SAMPLE_SIZE)\n",
    "    total_batches = (total_rows + batch_size - 1) // batch_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_tokens = 0\n",
    "    successful_embeddings = 0\n",
    "    \n",
    "    print(f\"   Processing {total_rows:,} rows in {total_batches} batches\")\n",
    "    print(\"   Processing first 5,000 rows...\")\n",
    "    \n",
    "    # Process first 5000 rows\n",
    "    embedding_records = []\n",
    "    \n",
    "    for batch_idx in range(0, min(5000, total_rows), batch_size):\n",
    "        batch_end = min(batch_idx + batch_size, min(5000, total_rows))\n",
    "        batch = data_pandas.iloc[batch_idx:batch_end]\n",
    "        current_batch = batch_idx // batch_size + 1\n",
    "        \n",
    "        if current_batch % 5 == 0:  # Progress update every 5 batches\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = successful_embeddings / elapsed if elapsed > 0 else 0\n",
    "            print(f\"   Batch {current_batch}/{min(50, total_batches)}: {successful_embeddings:,} embeddings \"\n",
    "                  f\"({rate:.1f}/sec), {total_tokens:,} tokens used\")\n",
    "        \n",
    "        # Get embeddings for batch\n",
    "        texts = batch['EMBEDDING_TEXT'].tolist()\n",
    "        embeddings, tokens = get_openai_embeddings_batch(texts, client)\n",
    "        \n",
    "        if embeddings:\n",
    "            total_tokens += tokens\n",
    "            \n",
    "            # Prepare records for insertion\n",
    "            for idx, (_, row) in enumerate(batch.iterrows()):\n",
    "                if idx < len(embeddings) and embeddings[idx]:\n",
    "                    embedding_records.append({\n",
    "                        'FDC_ID': int(row['FDC_ID']),\n",
    "                        'FOOD_NAME': row['FOOD_NAME'],\n",
    "                        'CATEGORY': row['CATEGORY'] if pd.notna(row['CATEGORY']) else None,\n",
    "                        'EMBEDDING_TEXT': row['EMBEDDING_TEXT'],\n",
    "                        'EMBEDDING': embeddings[idx],\n",
    "                        'GENERATION_TIME_MS': (time.time() - start_time) * 1000 / (successful_embeddings + 1)\n",
    "                    })\n",
    "                    successful_embeddings += 1\n",
    "        \n",
    "        # Add delay to respect rate limits\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"   First batch complete: {len(embedding_records):,} embeddings ready to insert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77985f-5321-44a7-81f9-aa7811e2033a",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "if client is not None and total_rows > 5000:\n",
    "    print(\"   Processing second 5,000 rows...\")\n",
    "    \n",
    "    # Process second 5000 rows\n",
    "    for batch_idx in range(5000, min(10000, total_rows), batch_size):\n",
    "        batch_end = min(batch_idx + batch_size, min(10000, total_rows))\n",
    "        batch = data_pandas.iloc[batch_idx:batch_end]\n",
    "        current_batch = batch_idx // batch_size + 1\n",
    "        \n",
    "        if current_batch % 5 == 0:  # Progress update\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = successful_embeddings / elapsed if elapsed > 0 else 0\n",
    "            print(f\"   Batch {current_batch}/{total_batches}: {successful_embeddings:,} embeddings \"\n",
    "                  f\"({rate:.1f}/sec), {total_tokens:,} tokens used\")\n",
    "        \n",
    "        # Get embeddings for batch\n",
    "        texts = batch['EMBEDDING_TEXT'].tolist()\n",
    "        embeddings, tokens = get_openai_embeddings_batch(texts, client)\n",
    "        \n",
    "        if embeddings:\n",
    "            total_tokens += tokens\n",
    "            \n",
    "            # Prepare records for insertion\n",
    "            for idx, (_, row) in enumerate(batch.iterrows()):\n",
    "                if idx < len(embeddings) and embeddings[idx]:\n",
    "                    embedding_records.append({\n",
    "                        'FDC_ID': int(row['FDC_ID']),\n",
    "                        'FOOD_NAME': row['FOOD_NAME'],\n",
    "                        'CATEGORY': row['CATEGORY'] if pd.notna(row['CATEGORY']) else None,\n",
    "                        'EMBEDDING_TEXT': row['EMBEDDING_TEXT'],\n",
    "                        'EMBEDDING': embeddings[idx],\n",
    "                        'GENERATION_TIME_MS': (time.time() - start_time) * 1000 / (successful_embeddings + 1)\n",
    "                    })\n",
    "                    successful_embeddings += 1\n",
    "        \n",
    "        # Add delay to respect rate limits\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Insert all embeddings to Snowflake\n",
    "    if embedding_records:\n",
    "        print(f\"\\nüì§ Inserting {len(embedding_records):,} embeddings to Snowflake...\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        embeddings_df = pd.DataFrame(embedding_records)\n",
    "        \n",
    "        # Create schema for Snowpark DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"FDC_ID\", LongType()),\n",
    "            StructField(\"FOOD_NAME\", StringType()),\n",
    "            StructField(\"CATEGORY\", StringType()),\n",
    "            StructField(\"EMBEDDING_TEXT\", StringType()),\n",
    "            StructField(\"EMBEDDING\", ArrayType(FloatType())),\n",
    "            StructField(\"GENERATION_TIME_MS\", FloatType())\n",
    "        ])\n",
    "        \n",
    "        # Create Snowpark DataFrame\n",
    "        snow_df = session.create_dataframe(embeddings_df, schema=schema)\n",
    "        \n",
    "        # Create temporary table\n",
    "        snow_df.create_or_replace_temp_view(\"temp_openai_embeddings\")\n",
    "        \n",
    "        # Insert with vector conversion\n",
    "        insert_count = session.sql(f\"\"\"\n",
    "            INSERT INTO {embeddings_schema}.OPENAI_EMBEDDINGS\n",
    "            SELECT \n",
    "                FDC_ID,\n",
    "                FOOD_NAME,\n",
    "                CATEGORY,\n",
    "                EMBEDDING_TEXT,\n",
    "                EMBEDDING::VECTOR(FLOAT, 1536) as EMBEDDING,\n",
    "                GENERATION_TIME_MS,\n",
    "                'text-embedding-3-small' as MODEL_NAME,\n",
    "                CURRENT_TIMESTAMP() as CREATED_AT\n",
    "            FROM temp_openai_embeddings\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        print(f\"‚úÖ Successfully inserted embeddings to table\")\n",
    "    \n",
    "    total_time = (time.time() - start_time)\n",
    "    print(f\"\\n‚úÖ OpenAI Embedding Generation Complete!\")\n",
    "    print(f\"   Total embeddings: {successful_embeddings:,}\")\n",
    "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"   Average time: {total_time/successful_embeddings*1000:.2f}ms per embedding\")\n",
    "    print(f\"   Total tokens used: {total_tokens:,}\")\n",
    "    print(f\"   Estimated cost: ${total_tokens * 0.00002:.4f} (@$0.020 per 1M tokens)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861ae78-b58c-44b3-86f2-57d7baee021c",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "print(\"üìä Calculating Performance Metrics...\")\n",
    "\n",
    "# Snowflake metrics\n",
    "snowflake_metrics = session.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as count,\n",
    "        AVG(GENERATION_TIME_MS) as avg_time,\n",
    "        MIN(GENERATION_TIME_MS) as min_time,\n",
    "        MAX(GENERATION_TIME_MS) as max_time,\n",
    "        STDDEV(GENERATION_TIME_MS) as std_time\n",
    "    FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "print(\"\\nüìç Snowflake Arctic Performance:\")\n",
    "print(f\"   Count: {snowflake_metrics['COUNT'][0]:,}\")\n",
    "print(f\"   Avg time: {snowflake_metrics['AVG_TIME'][0]:.2f}ms\")\n",
    "print(f\"   Min/Max: {snowflake_metrics['MIN_TIME'][0]:.2f}ms / {snowflake_metrics['MAX_TIME'][0]:.2f}ms\")\n",
    "\n",
    "# OpenAI metrics\n",
    "openai_metrics = session.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as count,\n",
    "        AVG(GENERATION_TIME_MS) as avg_time,\n",
    "        MIN(GENERATION_TIME_MS) as min_time,\n",
    "        MAX(GENERATION_TIME_MS) as max_time,\n",
    "        STDDEV(GENERATION_TIME_MS) as std_time\n",
    "    FROM {embeddings_schema}.OPENAI_EMBEDDINGS\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "print(\"\\nüìç OpenAI text-embedding-3-small Performance:\")\n",
    "print(f\"   Count: {openai_metrics['COUNT'][0]:,}\")\n",
    "if openai_metrics['COUNT'][0] > 0:\n",
    "    print(f\"   Avg time: {openai_metrics['AVG_TIME'][0]:.2f}ms\")\n",
    "    print(f\"   Min/Max: {openai_metrics['MIN_TIME'][0]:.2f}ms / {openai_metrics['MAX_TIME'][0]:.2f}ms\")\n",
    "    \n",
    "    # Speed comparison\n",
    "    if snowflake_metrics['AVG_TIME'][0] > 0 and openai_metrics['AVG_TIME'][0] > 0:\n",
    "        speed_ratio = openai_metrics['AVG_TIME'][0] / snowflake_metrics['AVG_TIME'][0]\n",
    "        print(f\"\\n‚ö° Snowflake is {speed_ratio:.2f}x faster than OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7352d-b874-4541-a02d-89f597378a4f",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "print(\"üíæ Storage Comparison...\")\n",
    "\n",
    "snowflake_count = snowflake_metrics['COUNT'][0]\n",
    "openai_count = openai_metrics['COUNT'][0]\n",
    "\n",
    "snowflake_storage_mb = snowflake_count * 768 * 4 / (1024 * 1024)\n",
    "openai_storage_mb = openai_count * 1536 * 4 / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìç Snowflake Arctic:\")\n",
    "print(f\"   Dimensions: 768\")\n",
    "print(f\"   Storage: {snowflake_storage_mb:.2f} MB for {snowflake_count:,} embeddings\")\n",
    "\n",
    "print(f\"\\nüìç OpenAI:\")\n",
    "print(f\"   Dimensions: 1536\")\n",
    "print(f\"   Storage: {openai_storage_mb:.2f} MB for {openai_count:,} embeddings\")\n",
    "\n",
    "if snowflake_storage_mb > 0 and openai_storage_mb > 0:\n",
    "    # Calculate per-embedding storage\n",
    "    snowflake_per_embedding = snowflake_storage_mb / snowflake_count * 1024  # in KB\n",
    "    openai_per_embedding = openai_storage_mb / openai_count * 1024  # in KB\n",
    "    \n",
    "    print(f\"\\nüìä Per-embedding storage:\")\n",
    "    print(f\"   Snowflake: {snowflake_per_embedding:.2f} KB\")\n",
    "    print(f\"   OpenAI: {openai_per_embedding:.2f} KB\")\n",
    "    \n",
    "    # If both have same count, calculate savings\n",
    "    if snowflake_count == openai_count:\n",
    "        savings = openai_storage_mb - snowflake_storage_mb\n",
    "        savings_pct = (1 - snowflake_storage_mb/openai_storage_mb) * 100\n",
    "        print(f\"\\nüí∞ Storage Savings with Snowflake: {savings:.2f} MB ({savings_pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d50a5-b346-4fad-8aa2-1201d082408e",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "def search_snowflake(query: str, top_k: int = 5):\n",
    "    \"\"\"Semantic search using Snowflake embeddings\"\"\"\n",
    "    try:\n",
    "        results = session.sql(f\"\"\"\n",
    "            WITH query_embed AS (\n",
    "                SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "                    'snowflake-arctic-embed-m', \n",
    "                    '{query.replace(\"'\", \"''\")}' \n",
    "                ) as QUERY_EMBEDDING\n",
    "            )\n",
    "            SELECT \n",
    "                s.FOOD_NAME,\n",
    "                s.CATEGORY,\n",
    "                VECTOR_COSINE_SIMILARITY(\n",
    "                    s.EMBEDDING, \n",
    "                    q.QUERY_EMBEDDING\n",
    "                ) as SIMILARITY\n",
    "            FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS s,\n",
    "                 query_embed q\n",
    "            ORDER BY SIMILARITY DESC\n",
    "            LIMIT {top_k}\n",
    "        \"\"\").to_pandas()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Snowflake search: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def search_openai(query: str, top_k: int = 5):\n",
    "    \"\"\"Semantic search using OpenAI embeddings\"\"\"\n",
    "    if client is None:\n",
    "        print(\"OpenAI search skipped - no API key\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Get query embedding\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        )\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        # Convert to string format for SQL\n",
    "        embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'\n",
    "        \n",
    "        results = session.sql(f\"\"\"\n",
    "            SELECT \n",
    "                FOOD_NAME,\n",
    "                CATEGORY,\n",
    "                VECTOR_COSINE_SIMILARITY(\n",
    "                    EMBEDDING,\n",
    "                    {embedding_str}::VECTOR(FLOAT, 1536)\n",
    "                ) as SIMILARITY\n",
    "            FROM {embeddings_schema}.OPENAI_EMBEDDINGS\n",
    "            ORDER BY SIMILARITY DESC\n",
    "            LIMIT {top_k}\n",
    "        \"\"\").to_pandas()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in OpenAI search: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"‚úÖ Search functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cbaf1-ce81-48f7-8a40-a5f2a5306740",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"healthy vegetarian high protein meals\",\n",
    "    \"low calorie breakfast options\",\n",
    "    \"quick snacks under 200 calories\",\n",
    "    \"mediterranean diet lunch\",\n",
    "    \"post workout recovery food\"\n",
    "]\n",
    "\n",
    "print(\"üîç Running Semantic Search Comparison...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overlap_scores = []\n",
    "\n",
    "for query in test_queries[:3]:  # Test first 3 queries\n",
    "    print(f\"\\nüìù Query: '{query}'\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Time Snowflake search\n",
    "    start = time.time()\n",
    "    snowflake_results = search_snowflake(query)\n",
    "    snowflake_time = (time.time() - start) * 1000\n",
    "    \n",
    "    if not snowflake_results.empty:\n",
    "        print(f\"\\nüìç Snowflake Arctic Results (Top 5) - {snowflake_time:.2f}ms:\")\n",
    "        for idx, row in snowflake_results.iterrows():\n",
    "            print(f\"   {idx+1}. {row['FOOD_NAME'][:40]:<40} Score: {row['SIMILARITY']:.4f}\")\n",
    "    \n",
    "    # Time OpenAI search\n",
    "    start = time.time()\n",
    "    openai_results = search_openai(query)\n",
    "    openai_time = (time.time() - start) * 1000\n",
    "    \n",
    "    if not openai_results.empty:\n",
    "        print(f\"\\nüìç OpenAI Results (Top 5) - {openai_time:.2f}ms:\")\n",
    "        for idx, row in openai_results.iterrows():\n",
    "            print(f\"   {idx+1}. {row['FOOD_NAME'][:40]:<40} Score: {row['SIMILARITY']:.4f}\")\n",
    "    \n",
    "    # Calculate overlap\n",
    "    if not snowflake_results.empty and not openai_results.empty:\n",
    "        snow_foods = set(snowflake_results['FOOD_NAME'].tolist())\n",
    "        openai_foods = set(openai_results['FOOD_NAME'].tolist())\n",
    "        overlap = len(snow_foods.intersection(openai_foods))\n",
    "        overlap_pct = overlap / 5 * 100\n",
    "        overlap_scores.append(overlap_pct)\n",
    "        \n",
    "        print(f\"\\nüìä Result Analysis:\")\n",
    "        print(f\"   ‚Ä¢ Common foods: {overlap}/5 ({overlap_pct:.0f}% overlap)\")\n",
    "        print(f\"   ‚Ä¢ Unique to Snowflake: {5-overlap}\")\n",
    "        print(f\"   ‚Ä¢ Unique to OpenAI: {5-overlap}\")\n",
    "        print(f\"   ‚Ä¢ Search times: Snowflake {snowflake_time:.2f}ms vs OpenAI {openai_time:.2f}ms\")\n",
    "\n",
    "if overlap_scores:\n",
    "    print(f\"\\nüìà Average overlap across all queries: {np.mean(overlap_scores):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a64f61-1ee6-4bfe-91f5-3e8e2957bcd6",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà FINAL COMPARISON REPORT - 10,000 EMBEDDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ GENERATION PERFORMANCE\")\n",
    "print(\"-\"*50)\n",
    "if snowflake_metrics['COUNT'][0] > 0:\n",
    "    print(f\"Snowflake Arctic:\")\n",
    "    print(f\"  ‚Ä¢ Embeddings generated: {snowflake_metrics['COUNT'][0]:,}\")\n",
    "    print(f\"  ‚Ä¢ Average generation time: {snowflake_metrics['AVG_TIME'][0]:.2f}ms\")\n",
    "    print(f\"  ‚Ä¢ Total storage: {snowflake_storage_mb:.2f} MB\")\n",
    "    print(f\"  ‚Ä¢ Cost: Included in Snowflake compute\")\n",
    "\n",
    "if openai_metrics['COUNT'][0] > 0:\n",
    "    print(f\"\\nOpenAI text-embedding-3-small:\")\n",
    "    print(f\"  ‚Ä¢ Embeddings generated: {openai_metrics['COUNT'][0]:,}\")\n",
    "    print(f\"  ‚Ä¢ Average generation time: {openai_metrics['AVG_TIME'][0]:.2f}ms\")\n",
    "    print(f\"  ‚Ä¢ Total storage: {openai_storage_mb:.2f} MB\")\n",
    "    if 'total_tokens' in locals():\n",
    "        print(f\"  ‚Ä¢ API cost: ${total_tokens * 0.00002:.4f} for {total_tokens:,} tokens\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ KEY PERFORMANCE INDICATORS\")\n",
    "print(\"-\"*50)\n",
    "if snowflake_metrics['COUNT'][0] > 0 and openai_metrics['COUNT'][0] > 0:\n",
    "    speed_advantage = openai_metrics['AVG_TIME'][0] / snowflake_metrics['AVG_TIME'][0]\n",
    "    storage_advantage = 2.0  # OpenAI uses 2x dimensions\n",
    "    \n",
    "    print(f\"‚ö° Speed: Snowflake is {speed_advantage:.1f}x faster\")\n",
    "    print(f\"üíæ Storage: Snowflake uses {1/storage_advantage:.1%} of OpenAI's storage\")\n",
    "    print(f\"üí∞ Cost: Snowflake has no API costs vs OpenAI's per-token pricing\")\n",
    "    if overlap_scores:\n",
    "        print(f\"üéØ Accuracy: {np.mean(overlap_scores):.0f}% average result overlap\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ QUALITATIVE COMPARISON\")\n",
    "print(\"-\"*50)\n",
    "print(\"‚úÖ Snowflake Arctic Advantages:\")\n",
    "print(\"  ‚Ä¢ Blazing fast generation (native integration)\")\n",
    "print(\"  ‚Ä¢ 50% less storage required\")\n",
    "print(\"  ‚Ä¢ No API key management\")\n",
    "print(\"  ‚Ä¢ No rate limits or quotas\")\n",
    "print(\"  ‚Ä¢ Included in compute costs\")\n",
    "\n",
    "print(\"\\n‚úÖ OpenAI Advantages:\")\n",
    "print(\"  ‚Ä¢ Higher dimensional embeddings (1536 vs 768)\")\n",
    "print(\"  ‚Ä¢ May capture more subtle semantic nuances\")\n",
    "print(\"  ‚Ä¢ Trained on broader internet corpus\")\n",
    "print(\"  ‚Ä¢ Better for cross-domain applications\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ RECOMMENDATIONS BY USE CASE\")\n",
    "print(\"-\"*50)\n",
    "print(\"üìå Choose Snowflake Arctic for:\")\n",
    "print(\"  ‚Ä¢ Production workloads at scale\")\n",
    "print(\"  ‚Ä¢ Real-time embedding generation\")\n",
    "print(\"  ‚Ä¢ Cost-sensitive deployments\")\n",
    "print(\"  ‚Ä¢ Applications staying within Snowflake\")\n",
    "print(\"  ‚Ä¢ Batch processing of large datasets\")\n",
    "\n",
    "print(\"\\nüìå Choose OpenAI for:\")\n",
    "print(\"  ‚Ä¢ Research requiring maximum accuracy\")\n",
    "print(\"  ‚Ä¢ Cross-domain semantic understanding\")\n",
    "print(\"  ‚Ä¢ Applications already using OpenAI APIs\")\n",
    "print(\"  ‚Ä¢ Small-scale prototypes (<1000 docs)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad712d4-0868-4472-a023-7c314289f2db",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96ecba-839e-413a-bb4e-3197ed407411",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "WITH query AS (\n",
    "    SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "        'snowflake-arctic-embed-m', \n",
    "        'high carb lunch'\n",
    "    ) as embedding\n",
    ")\n",
    "SELECT \n",
    "    FOOD_NAME,\n",
    "    CATEGORY,\n",
    "    VECTOR_COSINE_SIMILARITY(EMBEDDING, (SELECT embedding FROM query)) as SCORE\n",
    "FROM MEALMINDDATA.EMBEDDINGS_COMPARISON.SNOWFLAKE_EMBEDDINGS\n",
    "ORDER BY SCORE DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961c766-258b-412f-bad0-7a944fc5ac3b",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "# RAG with Snowflake Cortex LLM - Complete Pipeline\n",
    "def rag_query_with_llm(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline using Snowflake only:\n",
    "    1. Retrieve relevant context using Snowflake Arctic embeddings\n",
    "    2. Generate answer using Snowflake Cortex Complete (Mistral)\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        top_k: Number of documents to retrieve\n",
    "    \"\"\"\n",
    "    print(f\"üîç RAG Query: '{query}'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Retrieve relevant context using Snowflake embeddings\n",
    "    print(\"\\nüìö Step 1: Retrieving relevant context...\")\n",
    "    results = search_snowflake(query, top_k)\n",
    "    \n",
    "    if results.empty:\n",
    "        print(\"‚ùå No results found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Retrieved {len(results)} documents using Snowflake Arctic\")\n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"  {idx+1}. {row['FOOD_NAME']} (Score: {row['SIMILARITY']:.4f})\")\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\".join([\n",
    "        f\"- {row['FOOD_NAME']} (Category: {row['CATEGORY']}, Relevance Score: {row['SIMILARITY']:.4f})\"\n",
    "        for _, row in results.iterrows()\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Generate answer using Snowflake Cortex LLM\n",
    "    print(\"\\nü§ñ Step 2: Generating answer with Snowflake Cortex LLM...\")\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following food items from our database, answer the user's question.\n",
    "\n",
    "Context (Relevant Foods):\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Provide a helpful, create a meal that recommends specific foods from the context above. Include the food names and explain why they match the query.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Using Snowflake Cortex Complete function with Mistral-Large2\n",
    "        llm_query = f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "            'mistral-large2',\n",
    "            '{prompt.replace(\"'\", \"''\")}'\n",
    "        ) as RESPONSE\n",
    "        \"\"\"\n",
    "        \n",
    "        response = session.sql(llm_query).collect()[0]['RESPONSE']\n",
    "        \n",
    "        print(\"\\nüí¨ LLM Response:\")\n",
    "        print(\"-\"*70)\n",
    "        print(response)\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'retrieved_foods': results['FOOD_NAME'].tolist(),\n",
    "            'similarity_scores': results['SIMILARITY'].tolist(),\n",
    "            'llm_response': response,\n",
    "            'embedding_model': 'snowflake-arctic-embed-m',\n",
    "            'llm_model': 'mistral-large2'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating LLM response: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Batch RAG queries for testing\n",
    "def batch_rag_queries(queries: list, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Run multiple RAG queries and collect results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"üß™ Testing RAG System with Multiple Queries\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n\\n{'='*70}\")\n",
    "        print(f\"Query {i}/{len(queries)}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        result = rag_query_with_llm(query, top_k)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"\\n‚úÖ Successfully answered: '{query}'\")\n",
    "        \n",
    "        time.sleep(1)  # Brief pause between queries\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ RAG with Snowflake Cortex LLM functions ready!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  result = rag_query_with_llm('What are good high protein breakfast options?')\")\n",
    "print(\"  results = batch_rag_queries(['healthy snacks', 'low calorie lunch', 'post workout food'])\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1869148-78d7-4de5-9132-c3f0a2da0ac6",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "# Test RAG with various nutrition queries\n",
    "test_queries = [\n",
    "    \"What are good high protein breakfast options?\",\n",
    "    \"Suggest healthy low calorie snacks under 200 calories\",\n",
    "    \"What foods are good for post-workout recovery?\",\n",
    "    \"Mediterranean diet lunch ideas\"\n",
    "]\n",
    "\n",
    "# Run single query\n",
    "print(\"üß™ Single Query Test\")\n",
    "print(\"=\"*70)\n",
    "result = rag_query_with_llm(test_queries[1], top_k=5)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nüìä Query Results Summary:\")\n",
    "    print(f\"  Retrieved {len(result['retrieved_foods'])} relevant foods\")\n",
    "    print(f\"  Using: {result['embedding_model']} + {result['llm_model']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48f997-fbf1-4681-8a21-9e5d23d54468",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "# RAG with Meal Plan Generation - FIXED VERSION\n",
    "def rag_generate_meal_plan(\n",
    "    days: int = 2,\n",
    "    calorie_limit: int = 2000,\n",
    "    carb_limit: int = 200,\n",
    "    protein_target: int = None,\n",
    "    dietary_preferences: str = None,\n",
    "    top_k: int = 30\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a multi-day meal plan using RAG with Snowflake Cortex.\n",
    "    \n",
    "    Args:\n",
    "        days: Number of days for meal plan (default: 2)\n",
    "        calorie_limit: Daily calorie limit\n",
    "        carb_limit: Daily carb limit in grams\n",
    "        protein_target: Target protein in grams (optional)\n",
    "        dietary_preferences: e.g., \"vegetarian\", \"high-protein\", \"low-fat\"\n",
    "        top_k: Number of foods to retrieve from database\n",
    "    \"\"\"\n",
    "    print(f\"üçΩÔ∏è Generating {days}-Day Meal Plan\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìä Constraints: {calorie_limit} cal/day, {carb_limit}g carbs/day\")\n",
    "    if protein_target:\n",
    "        print(f\"ü•© Protein target: {protein_target}g/day\")\n",
    "    if dietary_preferences:\n",
    "        print(f\"ü•ó Preferences: {dietary_preferences}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Build search query based on preferences\n",
    "    if dietary_preferences:\n",
    "        search_query = f\"{dietary_preferences} meals breakfast lunch dinner snacks\"\n",
    "    else:\n",
    "        search_query = \"healthy balanced meals breakfast lunch dinner snacks\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant foods from database\n",
    "    print(f\"\\nüìö Step 1: Retrieving relevant foods from database...\")\n",
    "    results = search_snowflake(search_query, top_k=top_k)\n",
    "    \n",
    "    if results.empty:\n",
    "        print(\"‚ùå No results found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Retrieved {len(results)} relevant foods\")\n",
    "    \n",
    "    # Step 2: Get food names and fetch detailed nutritional info\n",
    "    print(\"\\nüìä Step 2: Fetching detailed nutritional information...\")\n",
    "    \n",
    "    food_names = results['FOOD_NAME'].tolist()\n",
    "    \n",
    "    # Build query to get nutrition details for these foods\n",
    "    food_names_escaped = [\"'\" + name.replace(\"'\", \"''\") + \"'\" for name in food_names]\n",
    "    food_names_str = ','.join(food_names_escaped)\n",
    "    \n",
    "    nutrition_query = f\"\"\"\n",
    "    SELECT \n",
    "        FOOD_NAME,\n",
    "        CATEGORY,\n",
    "        COALESCE(CALORIES, 0) as CALORIES,\n",
    "        COALESCE(PROTEIN, 0) as PROTEIN,\n",
    "        COALESCE(CARBOHYDRATE, 0) as CARBS,\n",
    "        COALESCE(TOTAL_FAT, 0) as FAT,\n",
    "        COALESCE(FIBER, 0) as FIBER\n",
    "    FROM {source_table}\n",
    "    WHERE FOOD_NAME IN ({food_names_str})\n",
    "    LIMIT {top_k}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        nutrition_data = session.sql(nutrition_query).to_pandas()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not fetch detailed nutrition. Using search results only.\")\n",
    "        nutrition_data = results\n",
    "    \n",
    "    # Build detailed context for LLM\n",
    "    food_context = []\n",
    "    for _, row in nutrition_data.iterrows():\n",
    "        food_name = row['FOOD_NAME']\n",
    "        category = row.get('CATEGORY', 'Unknown')\n",
    "        calories = row.get('CALORIES', 0)\n",
    "        protein = row.get('PROTEIN', 0)\n",
    "        carbs = row.get('CARBS', 0) if 'CARBS' in row else row.get('CARBOHYDRATE', 0)\n",
    "        fat = row.get('FAT', 0) if 'FAT' in row else row.get('TOTAL_FAT', 0)\n",
    "        fiber = row.get('FIBER', 0)\n",
    "        \n",
    "        food_context.append(\n",
    "            f\"- {food_name} (Category: {category}): \"\n",
    "            f\"{calories} cal, {protein}g protein, \"\n",
    "            f\"{carbs}g carbs, {fat}g fat, {fiber}g fiber\"\n",
    "        )\n",
    "    \n",
    "    context = \"\\n\".join(food_context)\n",
    "    \n",
    "    # Step 3: Build comprehensive prompt for meal plan generation\n",
    "    print(\"\\nü§ñ Step 3: Generating meal plan with LLM...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a professional nutritionist creating a detailed meal plan. \n",
    "\n",
    "AVAILABLE FOODS FROM DATABASE:\n",
    "{context}\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Create a {days}-day meal plan\n",
    "- Daily calorie limit: {calorie_limit} calories\n",
    "- Daily carb limit: {carb_limit} grams\n",
    "{f'- Daily protein target: {protein_target} grams' if protein_target else ''}\n",
    "{f'- Dietary preference: {dietary_preferences}' if dietary_preferences else ''}\n",
    "- Must include: Breakfast, Lunch, Snacks, Dinner for each day\n",
    "- ONLY use foods from the available database list above\n",
    "- Stay within the daily limits\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "For each day, provide:\n",
    "\n",
    "**Day [X]:**\n",
    "\n",
    "**Breakfast:**\n",
    "- [Food name] ([portion size])\n",
    "  Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "**Lunch:**\n",
    "- [Food name] ([portion size])\n",
    "  Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "**Snacks:**\n",
    "- [Food name] ([portion size])\n",
    "  Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "**Dinner:**\n",
    "- [Food name] ([portion size])\n",
    "  Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "**Day [X] Totals:**\n",
    "- Total Calories: [X] / {calorie_limit} cal\n",
    "- Total Protein: [X]g\n",
    "- Total Carbs: [X] / {carb_limit}g\n",
    "- Total Fat: [X]g\n",
    "- Status: [Within/Over] limits\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Select appropriate portion sizes to meet but not exceed limits\n",
    "2. Distribute calories reasonably across meals (breakfast: ~25%, lunch: ~30%, snacks: ~15%, dinner: ~30%)\n",
    "3. Calculate exact totals for each day\n",
    "4. Verify daily totals are within constraints\n",
    "5. Use realistic portion sizes (e.g., \"1 cup\", \"100g\", \"1 medium\", \"2 pieces\")\n",
    "6. Choose foods that complement each other for balanced nutrition\n",
    "7. Only use foods explicitly listed in the available database\n",
    "\n",
    "Generate the complete {days}-day meal plan now:\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Call Snowflake Cortex Complete\n",
    "        llm_query = f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "            'mistral-large2',\n",
    "            '{prompt.replace(\"'\", \"''\")}'\n",
    "        ) as MEAL_PLAN\n",
    "        \"\"\"\n",
    "        \n",
    "        response = session.sql(llm_query).collect()[0]['MEAL_PLAN']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"üìã {days}-DAY MEAL PLAN\")\n",
    "        print(\"=\"*70)\n",
    "        print(response)\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'days': days,\n",
    "            'calorie_limit': calorie_limit,\n",
    "            'carb_limit': carb_limit,\n",
    "            'protein_target': protein_target,\n",
    "            'dietary_preferences': dietary_preferences,\n",
    "            'available_foods_count': len(nutrition_data),\n",
    "            'meal_plan': response,\n",
    "            'nutrition_data': nutrition_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating meal plan: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Convenience function with common presets\n",
    "def quick_meal_plan(preset: str = \"balanced\"):\n",
    "    \"\"\"\n",
    "    Generate meal plans with common presets\n",
    "    \n",
    "    Presets:\n",
    "    - \"balanced\": 2000 cal, 200g carbs\n",
    "    - \"low_carb\": 1800 cal, 100g carbs, high protein\n",
    "    - \"high_protein\": 2200 cal, 180g carbs, 150g protein\n",
    "    - \"weight_loss\": 1500 cal, 150g carbs\n",
    "    - \"vegetarian\": 2000 cal, 220g carbs\n",
    "    \"\"\"\n",
    "    presets = {\n",
    "        \"balanced\": {\n",
    "            \"days\": 2,\n",
    "            \"calorie_limit\": 2000,\n",
    "            \"carb_limit\": 200,\n",
    "            \"protein_target\": 100,\n",
    "            \"dietary_preferences\": None\n",
    "        },\n",
    "        \"low_carb\": {\n",
    "            \"days\": 2,\n",
    "            \"calorie_limit\": 1800,\n",
    "            \"carb_limit\": 100,\n",
    "            \"protein_target\": 120,\n",
    "            \"dietary_preferences\": \"high protein low carb\"\n",
    "        },\n",
    "        \"high_protein\": {\n",
    "            \"days\": 2,\n",
    "            \"calorie_limit\": 2200,\n",
    "            \"carb_limit\": 180,\n",
    "            \"protein_target\": 150,\n",
    "            \"dietary_preferences\": \"high protein\"\n",
    "        },\n",
    "        \"weight_loss\": {\n",
    "            \"days\": 2,\n",
    "            \"calorie_limit\": 1500,\n",
    "            \"carb_limit\": 150,\n",
    "            \"protein_target\": 90,\n",
    "            \"dietary_preferences\": \"low calorie healthy\"\n",
    "        },\n",
    "        \"vegetarian\": {\n",
    "            \"days\": 2,\n",
    "            \"calorie_limit\": 2000,\n",
    "            \"carb_limit\": 220,\n",
    "            \"protein_target\": 80,\n",
    "            \"dietary_preferences\": \"vegetarian plant-based\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if preset not in presets:\n",
    "        print(f\"‚ùå Unknown preset. Available: {list(presets.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    config = presets[preset]\n",
    "    return rag_generate_meal_plan(**config)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Meal Plan Generation functions ready!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  # Custom meal plan\")\n",
    "print(\"  plan = rag_generate_meal_plan(days=2, calorie_limit=2000, carb_limit=200, protein_target=100)\")\n",
    "print(\"\\n  # Quick presets\")\n",
    "print(\"  plan = quick_meal_plan('balanced')\")\n",
    "print(\"  plan = quick_meal_plan('low_carb')\")\n",
    "print(\"  plan = quick_meal_plan('high_protein')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f3554-014b-4cdf-9238-c7abb4d28ff1",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "# Test with custom parameters\n",
    "result = rag_generate_meal_plan(\n",
    "    days=2,\n",
    "    calorie_limit=2000,\n",
    "    carb_limit=200,\n",
    "    protein_target=100,\n",
    "    dietary_preferences=\"balanced healthy\",\n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n‚úÖ Meal plan generated successfully!\")\n",
    "    print(f\"üìä Used {result['available_foods_count']} foods from database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f178a-3bb3-4923-abf3-faff3066edc0",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "# Meal Plan Generator from Personal Inventory\n",
    "def rag_generate_meal_plan_from_inventory(\n",
    "    inventory: list,\n",
    "    days: int = 2,\n",
    "    calorie_limit: int = 2000,\n",
    "    carb_limit: int = 200,\n",
    "    protein_target: int = None,\n",
    "    dietary_preferences: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a meal plan using ONLY foods from your inventory.\n",
    "    Fetches exact nutritional data from the database.\n",
    "    \n",
    "    Args:\n",
    "        inventory: List of food names you have available\n",
    "                  Example: [\"eggs\", \"chicken breast\", \"brown rice\", \"spinach\", ...]\n",
    "        days: Number of days for meal plan (default: 2)\n",
    "        calorie_limit: Daily calorie limit\n",
    "        carb_limit: Daily carb limit in grams\n",
    "        protein_target: Target protein in grams (optional)\n",
    "        dietary_preferences: Additional preferences\n",
    "    \"\"\"\n",
    "    print(f\"üçΩÔ∏è Generating {days}-Day Meal Plan from YOUR Inventory\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üì¶ Inventory items: {len(inventory)}\")\n",
    "    print(f\"üìä Constraints: {calorie_limit} cal/day, {carb_limit}g carbs/day\")\n",
    "    if protein_target:\n",
    "        print(f\"ü•© Protein target: {protein_target}g/day\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Find matching foods in database using semantic search\n",
    "    print(f\"\\nüìö Step 1: Finding your foods in database...\")\n",
    "    \n",
    "    matched_foods = []\n",
    "    food_search_results = {}\n",
    "    \n",
    "    for food_item in inventory:\n",
    "        # Search database for this inventory item\n",
    "        results = search_snowflake(food_item, top_k=3)\n",
    "        if not results.empty:\n",
    "            # Take the best match (highest similarity)\n",
    "            best_match = results.iloc[0]\n",
    "            if best_match['SIMILARITY'] > 0.6:  # Good match threshold\n",
    "                matched_foods.append(best_match['FOOD_NAME'])\n",
    "                food_search_results[food_item] = best_match['FOOD_NAME']\n",
    "                print(f\"  ‚úì '{food_item}' ‚Üí '{best_match['FOOD_NAME']}' (score: {best_match['SIMILARITY']:.3f})\")\n",
    "            else:\n",
    "                print(f\"  ‚ö† '{food_item}' - no good match found\")\n",
    "    \n",
    "    if len(matched_foods) < 5:\n",
    "        print(f\"\\n‚ùå Error: Only found {len(matched_foods)} foods in database. Need at least 5.\")\n",
    "        print(\"   Please check your inventory items or add more foods.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Matched {len(matched_foods)} foods from your inventory\")\n",
    "    \n",
    "    # Step 2: Fetch detailed nutrition data for matched foods\n",
    "    print(f\"\\nüìä Step 2: Fetching nutritional data from database...\")\n",
    "    \n",
    "    food_names_escaped = [\"'\" + name.replace(\"'\", \"''\") + \"'\" for name in matched_foods]\n",
    "    food_names_str = ','.join(food_names_escaped)\n",
    "    \n",
    "    nutrition_query = f\"\"\"\n",
    "    SELECT \n",
    "        FOOD_NAME,\n",
    "        CATEGORY,\n",
    "        COALESCE(CALORIES, 0) as CALORIES,\n",
    "        COALESCE(PROTEIN, 0) as PROTEIN,\n",
    "        COALESCE(CARBOHYDRATE, 0) as CARBS,\n",
    "        COALESCE(TOTAL_FAT, 0) as FAT,\n",
    "        COALESCE(FIBER, 0) as FIBER,\n",
    "        COALESCE(SODIUM, 0) as SODIUM\n",
    "    FROM {source_table}\n",
    "    WHERE FOOD_NAME IN ({food_names_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        nutrition_data = session.sql(nutrition_query).to_pandas()\n",
    "        print(f\"‚úÖ Retrieved nutrition data for {len(nutrition_data)} foods\")\n",
    "        \n",
    "        # Show sample of nutrition data\n",
    "        print(\"\\nüìã Sample nutrition data from your inventory:\")\n",
    "        for idx, row in nutrition_data.head(5).iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['FOOD_NAME'][:40]}: {row['CALORIES']}cal, \"\n",
    "                  f\"{row['PROTEIN']}g protein, {row['CARBS']}g carbs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching nutrition data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Build context for LLM with exact nutrition values\n",
    "    print(\"\\nüìù Step 3: Building meal plan context...\")\n",
    "    \n",
    "    food_context = []\n",
    "    for _, row in nutrition_data.iterrows():\n",
    "        food_context.append(\n",
    "            f\"- {row['FOOD_NAME']} (Category: {row['CATEGORY']}): \"\n",
    "            f\"{row['CALORIES']} cal, {row['PROTEIN']}g protein, \"\n",
    "            f\"{row['CARBS']}g carbs, {row['FAT']}g fat, {row['FIBER']}g fiber\"\n",
    "        )\n",
    "    \n",
    "    context = \"\\n\".join(food_context)\n",
    "    \n",
    "    # Step 4: Generate meal plan using Snowflake Cortex\n",
    "    print(\"\\nü§ñ Step 4: Generating personalized meal plan...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a professional meal planner creating a personalized {days}-day meal plan.\n",
    "\n",
    "AVAILABLE FOODS (From User's Inventory - with exact nutritional data):\n",
    "{context}\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Create a {days}-day meal plan\n",
    "- Daily calorie limit: {calorie_limit} calories\n",
    "- Daily carb limit: {carb_limit} grams\n",
    "{f'- Daily protein target: {protein_target} grams' if protein_target else ''}\n",
    "{f'- Dietary preference: {dietary_preferences}' if dietary_preferences else ''}\n",
    "- Must include: Breakfast, Lunch, Snacks, Dinner for each day\n",
    "- ONLY use foods from the user's available inventory above\n",
    "- Use the EXACT nutritional values provided\n",
    "- Stay within daily limits\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "For each day, provide:\n",
    "\n",
    "**Day [X]:**\n",
    "\n",
    "**Breakfast:**\n",
    "- [Food name] ([portion size, e.g., \"100g\", \"1 cup\", \"2 pieces\"])\n",
    "  Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "**Lunch:**\n",
    "- [Food name] ([portion size])\n",
    "  Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "**Snacks:**\n",
    "- [Food name] ([portion size])\n",
    "  Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "**Dinner:**\n",
    "- [Food name] ([portion size])\n",
    "  Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "**Day [X] Totals:**\n",
    "- Total Calories: [X] / {calorie_limit} cal ({\"Within\" if \"[X]\" else \"Over\"} limit)\n",
    "- Total Protein: [X]g\n",
    "- Total Carbs: [X] / {carb_limit}g ({\"Within\" if \"[X]\" else \"Over\"} limit)\n",
    "- Total Fat: [X]g\n",
    "- Status: ‚úì Within limits / ‚úó Over limits\n",
    "\n",
    "CALCULATION RULES:\n",
    "1. Use the exact nutritional values per 100g from the database\n",
    "2. Scale proportionally based on portion size (e.g., 150g = 1.5x the 100g values)\n",
    "3. Sum all meal components accurately\n",
    "4. Verify totals are within constraints\n",
    "5. Distribute calories: breakfast 25%, lunch 30%, snacks 15%, dinner 30%\n",
    "6. Use realistic, measurable portions\n",
    "7. Vary foods across days when possible\n",
    "\n",
    "Generate the complete {days}-day meal plan with precise calculations:\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Call Snowflake Cortex Complete\n",
    "        llm_query = f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "            'mistral-large2',\n",
    "            '{prompt.replace(\"'\", \"''\")}'\n",
    "        ) as MEAL_PLAN\n",
    "        \"\"\"\n",
    "        \n",
    "        response = session.sql(llm_query).collect()[0]['MEAL_PLAN']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"üìã YOUR PERSONALIZED {days}-DAY MEAL PLAN\")\n",
    "        print(\"=\"*70)\n",
    "        print(response)\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Show mapping of what was used\n",
    "        print(f\"\\nüì¶ Inventory Mapping:\")\n",
    "        print(\"-\"*70)\n",
    "        for user_item, db_food in food_search_results.items():\n",
    "            print(f\"  '{user_item}' ‚Üí '{db_food}'\")\n",
    "        \n",
    "        return {\n",
    "            'days': days,\n",
    "            'calorie_limit': calorie_limit,\n",
    "            'carb_limit': carb_limit,\n",
    "            'protein_target': protein_target,\n",
    "            'user_inventory': inventory,\n",
    "            'matched_foods': matched_foods,\n",
    "            'inventory_mapping': food_search_results,\n",
    "            'available_foods_count': len(nutrition_data),\n",
    "            'meal_plan': response,\n",
    "            'nutrition_data': nutrition_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating meal plan: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Inventory-based meal plan generator ready!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"\"\"\n",
    "my_inventory = [\n",
    "    \"eggs\", \"chicken breast\", \"salmon\", \n",
    "    \"brown rice\", \"quinoa\", \"sweet potato\",\n",
    "    \"spinach\", \"broccoli\", \"carrots\",\n",
    "    \"greek yogurt\", \"almonds\", \"banana\",\n",
    "    \"avocado\", \"olive oil\", \"oatmeal\"\n",
    "]\n",
    "\n",
    "plan = rag_generate_meal_plan_from_inventory(\n",
    "    inventory=my_inventory,\n",
    "    days=2,\n",
    "    calorie_limit=2000,\n",
    "    carb_limit=200,\n",
    "    protein_target=120\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d430c5-7ade-4b8c-bee4-a046f8ff7625",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "# Define YOUR available foods\n",
    "my_kitchen_inventory = [\n",
    "    # Proteins\n",
    "    \"eggs\",\n",
    "    \"chicken breast\", \n",
    "    \"salmon\",\n",
    "    \"greek yogurt\",\n",
    "    \"cottage cheese\",\n",
    "    \"tofu\",\n",
    "    \n",
    "    # Carbs\n",
    "    \"brown rice\",\n",
    "    \"quinoa\",\n",
    "    \"oatmeal\",\n",
    "    \"whole wheat bread\",\n",
    "    \"sweet potato\",\n",
    "    \"pasta\",\n",
    "    \n",
    "    # Vegetables\n",
    "    \"spinach\",\n",
    "    \"broccoli\",\n",
    "    \"carrots\",\n",
    "    \"bell peppers\",\n",
    "    \"tomatoes\",\n",
    "    \"cucumber\",\n",
    "    \n",
    "    # Fruits\n",
    "    \"banana\",\n",
    "    \"apple\",\n",
    "    \"berries\",\n",
    "    \"orange\",\n",
    "    \n",
    "    # Healthy fats\n",
    "    \"avocado\",\n",
    "    \"almonds\",\n",
    "    \"olive oil\",\n",
    "    \"peanut butter\",\n",
    "    \n",
    "    # Other\n",
    "    \"milk\",\n",
    "    \"cheese\",\n",
    "    \"beans\"\n",
    "]\n",
    "\n",
    "# Generate meal plan\n",
    "result = rag_generate_meal_plan_from_inventory(\n",
    "    inventory=my_kitchen_inventory,\n",
    "    days=2,\n",
    "    calorie_limit=2000,\n",
    "    carb_limit=200,\n",
    "    protein_target=120,\n",
    "    dietary_preferences=\"balanced healthy\"\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n‚úÖ Success! Meal plan created using {result['available_foods_count']} foods from your inventory\")\n",
    "    \n",
    "    # Save nutrition data for reference\n",
    "    print(\"\\nüìä Nutrition reference table:\")\n",
    "    print(result['nutrition_data'][['FOOD_NAME', 'CALORIES', 'PROTEIN', 'CARBS', 'FAT']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129cb10-de34-488a-9b1a-43cf5823627d",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "# Meal Plan Generator from Inventory WITH RECIPES\n",
    "def rag_generate_meal_plan_with_recipes(\n",
    "    inventory: list,\n",
    "    days: int = 2,\n",
    "    calorie_limit: int = 2000,\n",
    "    carb_limit: int = 200,\n",
    "    protein_target: int = None,\n",
    "    dietary_preferences: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a meal plan with RECIPES using foods from your inventory.\n",
    "    Includes cooking instructions, prep time, and nutrition calculations.\n",
    "    \n",
    "    Args:\n",
    "        inventory: List of food names you have available\n",
    "        days: Number of days for meal plan\n",
    "        calorie_limit: Daily calorie limit\n",
    "        carb_limit: Daily carb limit in grams\n",
    "        protein_target: Target protein in grams (optional)\n",
    "        dietary_preferences: Additional preferences\n",
    "    \"\"\"\n",
    "    print(f\"üçΩÔ∏è Generating {days}-Day Meal Plan WITH RECIPES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üì¶ Inventory items: {len(inventory)}\")\n",
    "    print(f\"üìä Constraints: {calorie_limit} cal/day, {carb_limit}g carbs/day\")\n",
    "    if protein_target:\n",
    "        print(f\"ü•© Protein target: {protein_target}g/day\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Find matching foods in database\n",
    "    print(f\"\\nüìö Step 1: Finding your foods in database...\")\n",
    "    \n",
    "    matched_foods = []\n",
    "    food_search_results = {}\n",
    "    \n",
    "    for food_item in inventory:\n",
    "        results = search_snowflake(food_item, top_k=3)\n",
    "        if not results.empty:\n",
    "            best_match = results.iloc[0]\n",
    "            if best_match['SIMILARITY'] > 0.6:\n",
    "                matched_foods.append(best_match['FOOD_NAME'])\n",
    "                food_search_results[food_item] = best_match['FOOD_NAME']\n",
    "                print(f\"  ‚úì '{food_item}' ‚Üí '{best_match['FOOD_NAME']}' (score: {best_match['SIMILARITY']:.3f})\")\n",
    "    \n",
    "    if len(matched_foods) < 5:\n",
    "        print(f\"\\n‚ùå Error: Only found {len(matched_foods)} foods. Need at least 5.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Matched {len(matched_foods)} foods from your inventory\")\n",
    "    \n",
    "    # Step 2: Fetch nutrition data\n",
    "    print(f\"\\nüìä Step 2: Fetching nutritional data...\")\n",
    "    \n",
    "    food_names_escaped = [\"'\" + name.replace(\"'\", \"''\") + \"'\" for name in matched_foods]\n",
    "    food_names_str = ','.join(food_names_escaped)\n",
    "    \n",
    "    nutrition_query = f\"\"\"\n",
    "    SELECT \n",
    "        FOOD_NAME,\n",
    "        CATEGORY,\n",
    "        COALESCE(CALORIES, 0) as CALORIES,\n",
    "        COALESCE(PROTEIN, 0) as PROTEIN,\n",
    "        COALESCE(CARBOHYDRATE, 0) as CARBS,\n",
    "        COALESCE(TOTAL_FAT, 0) as FAT,\n",
    "        COALESCE(FIBER, 0) as FIBER\n",
    "    FROM {source_table}\n",
    "    WHERE FOOD_NAME IN ({food_names_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        nutrition_data = session.sql(nutrition_query).to_pandas()\n",
    "        print(f\"‚úÖ Retrieved nutrition data for {len(nutrition_data)} foods\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Build context\n",
    "    food_context = []\n",
    "    for _, row in nutrition_data.iterrows():\n",
    "        food_context.append(\n",
    "            f\"- {row['FOOD_NAME']} (Category: {row['CATEGORY']}): \"\n",
    "            f\"{row['CALORIES']} cal, {row['PROTEIN']}g protein, \"\n",
    "            f\"{row['CARBS']}g carbs, {row['FAT']}g fat per 100g\"\n",
    "        )\n",
    "    \n",
    "    context = \"\\n\".join(food_context)\n",
    "    \n",
    "    # Step 4: Generate meal plan with recipes\n",
    "    print(\"\\nü§ñ Step 3: Generating meal plan with detailed recipes...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a professional chef and nutritionist creating a detailed {days}-day meal plan with complete recipes.\n",
    "\n",
    "AVAILABLE INGREDIENTS (From User's Kitchen):\n",
    "{context}\n",
    "All the items in the database are per 100g quantitative measure.\n",
    "REQUIREMENTS:\n",
    "- Create {days}-day meal plan with COMPLETE RECIPES\n",
    "- Daily calorie limit: {calorie_limit} calories\n",
    "- Daily carb limit: {carb_limit} grams\n",
    "{f'- Daily protein target: {protein_target} grams' if protein_target else ''}\n",
    "{f'- Dietary preference: {dietary_preferences}' if dietary_preferences else ''}\n",
    "- Include: Breakfast, Lunch, Snacks, Dinner for each day\n",
    "- ONLY use ingredients from the available list above\n",
    "- Provide detailed cooking instructions for each meal\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "\n",
    "**Day [X]:**\n",
    "\n",
    "---\n",
    "\n",
    "### Breakfast: [Recipe Name]\n",
    "**Prep Time:** [X] minutes | **Cook Time:** [X] minutes\n",
    "\n",
    "**Ingredients:**\n",
    "- [Ingredient 1] ([amount])\n",
    "- [Ingredient 2] ([amount])\n",
    "- [Ingredient 3] ([amount])\n",
    "\n",
    "**Cooking Instructions:**\n",
    "1. [Step 1 with specific details]\n",
    "2. [Step 2 with temperature/time]\n",
    "3. [Step 3 with cooking technique]\n",
    "4. [Final step]\n",
    "\n",
    "**Nutrition (per serving):**\n",
    "Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "---\n",
    "\n",
    "### Lunch: [Recipe Name]\n",
    "**Prep Time:** [X] minutes | **Cook Time:** [X] minutes\n",
    "\n",
    "**Ingredients:**\n",
    "- [Ingredient 1] ([amount])\n",
    "- [Ingredient 2] ([amount])\n",
    "\n",
    "**Cooking Instructions:**\n",
    "1. [Detailed step 1]\n",
    "2. [Detailed step 2]\n",
    "3. [Detailed step 3]\n",
    "\n",
    "**Nutrition (per serving):**\n",
    "Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "---\n",
    "\n",
    "### Snacks: [Recipe Name]\n",
    "**Prep Time:** [X] minutes\n",
    "\n",
    "**Ingredients:**\n",
    "- [Ingredient 1] ([amount])\n",
    "- [Ingredient 2] ([amount])\n",
    "\n",
    "**Instructions:**\n",
    "1. [Simple preparation step]\n",
    "\n",
    "**Nutrition:**\n",
    "Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "---\n",
    "\n",
    "### Dinner: [Recipe Name]\n",
    "**Prep Time:** [X] minutes | **Cook Time:** [X] minutes\n",
    "\n",
    "**Ingredients:**\n",
    "- [Ingredient 1] ([amount])\n",
    "- [Ingredient 2] ([amount])\n",
    "- [Ingredient 3] ([amount])\n",
    "\n",
    "**Cooking Instructions:**\n",
    "1. [Detailed step 1]\n",
    "2. [Detailed step 2]\n",
    "3. [Detailed step 3]\n",
    "4. [Final step]\n",
    "\n",
    "**Nutrition (per serving):**\n",
    "Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "---\n",
    "\n",
    "**Day [X] Totals:**\n",
    "- Total Calories: [X] / {calorie_limit} cal\n",
    "- Total Protein: [X]g\n",
    "- Total Carbs: [X] / {carb_limit}g\n",
    "- Total Fat: [X]g\n",
    "- Status: ‚úì Within limits\n",
    "\n",
    "---\n",
    "\n",
    "IMPORTANT RECIPE GUIDELINES:\n",
    "1. Give each meal a creative, appetizing name\n",
    "2. List realistic prep and cook times\n",
    "3. Provide step-by-step cooking instructions with temperatures and techniques\n",
    "4. Use exact measurements (cups, grams, tablespoons)\n",
    "5. Scale nutrition values based on portion sizes\n",
    "6. Make instructions clear and easy to follow\n",
    "7. Include cooking methods (saut√©, bake, grill, steam, etc.)\n",
    "8. Suggest seasoning and flavor enhancements using available ingredients\n",
    "9. Keep recipes simple and practical for home cooking\n",
    "10. Calculate accurate nutrition totals for each day\n",
    "\n",
    "Generate the complete {days}-day meal plan with detailed recipes now:\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_query = f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "            'mistral-large2',\n",
    "            '{prompt.replace(\"'\", \"''\")}'\n",
    "        ) as MEAL_PLAN\n",
    "        \"\"\"\n",
    "        \n",
    "        response = session.sql(llm_query).collect()[0]['MEAL_PLAN']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"üìã YOUR {days}-DAY MEAL PLAN WITH RECIPES\")\n",
    "        print(\"=\"*70)\n",
    "        print(response)\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'days': days,\n",
    "            'calorie_limit': calorie_limit,\n",
    "            'carb_limit': carb_limit,\n",
    "            'protein_target': protein_target,\n",
    "            'user_inventory': inventory,\n",
    "            'matched_foods': matched_foods,\n",
    "            'inventory_mapping': food_search_results,\n",
    "            'available_foods_count': len(nutrition_data),\n",
    "            'meal_plan_with_recipes': response,\n",
    "            'nutrition_data': nutrition_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Recipe-based meal plan generator ready!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"\"\"\n",
    "my_inventory = [\n",
    "    \"eggs\", \"chicken breast\", \"salmon\", \"tofu\",\n",
    "    \"brown rice\", \"quinoa\", \"oatmeal\", \"pasta\",\n",
    "    \"spinach\", \"broccoli\", \"tomatoes\", \"bell peppers\",\n",
    "    \"greek yogurt\", \"milk\", \"cheese\", \n",
    "    \"almonds\", \"olive oil\", \"garlic\", \"onions\"\n",
    "]\n",
    "\n",
    "plan = rag_generate_meal_plan_with_recipes(\n",
    "    inventory=my_inventory,\n",
    "    days=2,\n",
    "    calorie_limit=2000,\n",
    "    carb_limit=200,\n",
    "    protein_target=120,\n",
    "    dietary_preferences=\"balanced healthy\"\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77183d46-954f-4191-8283-8fcb7b75a06f",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "# Define your kitchen inventory\n",
    "my_kitchen = [\n",
    "    # Proteins\n",
    "    \"eggs\", \"chicken breast\", \"Whey Protein (22g per scoup)\", \"ground turkey\",\n",
    "    \"greek yogurt\", \"cottage cheese\", \"tofu\", \"chickpeas\",\n",
    "    \n",
    "    # Grains & Carbs\n",
    "    \"brown rice\", \"quinoa\", \"oatmeal\", \"whole wheat pasta\",\n",
    "    \"sweet potato\", \"whole wheat bread\", \"tortillas\",\n",
    "    \n",
    "    # Vegetables\n",
    "    \"spinach\", \"broccoli\", \"carrots\", \"bell peppers\",\n",
    "    \"tomatoes\", \"cucumber\", \"zucchini\", \"kale\",\n",
    "    \"onions\", \"garlic\", \"mushrooms\",\n",
    "    \n",
    "    # Fruits\n",
    "    \"banana\", \"apple\", \"berries\", \"orange\", \"avocado\",\n",
    "    \n",
    "    # Healthy Fats & Others\n",
    "    \"almonds\", \"walnuts\", \"peanut butter\", \"olive oil\",\n",
    "    \"coconut oil\", \"cheese\", \"milk\", \"beans\",\n",
    "    \"lentils\", \"soy sauce\", \"honey\"\n",
    "]\n",
    "\n",
    "# Generate 2-day meal plan with complete recipes\n",
    "result = rag_generate_meal_plan_with_recipes(\n",
    "    inventory=my_kitchen,\n",
    "    days=2,\n",
    "    calorie_limit=2000,\n",
    "    carb_limit=200,\n",
    "    protein_target=120,\n",
    "    dietary_preferences=\"balanced healthy meals\"\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n‚úÖ Complete meal plan with recipes generated!\")\n",
    "    print(f\"üìä Using {result['available_foods_count']} ingredients from your kitchen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb9c23-3eea-49d9-a28d-719155e47ee8",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "# Meal Plan Generator WITH USER HABITS AND PREFERENCES\n",
    "def rag_generate_meal_plan_with_habits(\n",
    "    inventory: list,\n",
    "    days: int = 2,\n",
    "    calorie_limit: int = 2000,\n",
    "    carb_limit: int = 200,\n",
    "    protein_target: int = None,\n",
    "    dietary_preferences: str = None,\n",
    "    daily_habits: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a meal plan with RECIPES that incorporates user's daily habits.\n",
    "    \n",
    "    Args:\n",
    "        inventory: List of food names you have available\n",
    "        days: Number of days for meal plan\n",
    "        calorie_limit: Daily calorie limit\n",
    "        carb_limit: Daily carb limit in grams\n",
    "        protein_target: Target protein in grams (optional)\n",
    "        dietary_preferences: Additional preferences\n",
    "        daily_habits: Dictionary of fixed habits, e.g.:\n",
    "                     {\n",
    "                         \"breakfast\": \"whey protein milkshake\",\n",
    "                         \"post_workout\": \"banana and peanut butter\",\n",
    "                         \"snack\": \"greek yogurt with berries\",\n",
    "                         \"avoid\": [\"dairy\", \"gluten\"]\n",
    "                     }\n",
    "    \"\"\"\n",
    "    print(f\"üçΩÔ∏è Generating {days}-Day Meal Plan WITH HABITS & RECIPES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üì¶ Inventory items: {len(inventory)}\")\n",
    "    print(f\"üìä Constraints: {calorie_limit} cal/day, {carb_limit}g carbs/day\")\n",
    "    if protein_target:\n",
    "        print(f\"ü•© Protein target: {protein_target}g/day\")\n",
    "    if daily_habits:\n",
    "        print(f\"üîÑ Daily habits: {len(daily_habits)} configured\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Find matching foods in database\n",
    "    print(f\"\\nüìö Step 1: Finding your foods in database...\")\n",
    "    \n",
    "    matched_foods = []\n",
    "    food_search_results = {}\n",
    "    \n",
    "    for food_item in inventory:\n",
    "        results = search_snowflake(food_item, top_k=3)\n",
    "        if not results.empty:\n",
    "            best_match = results.iloc[0]\n",
    "            if best_match['SIMILARITY'] > 0.6:\n",
    "                matched_foods.append(best_match['FOOD_NAME'])\n",
    "                food_search_results[food_item] = best_match['FOOD_NAME']\n",
    "                print(f\"  ‚úì '{food_item}' ‚Üí '{best_match['FOOD_NAME']}' (score: {best_match['SIMILARITY']:.3f})\")\n",
    "    \n",
    "    if len(matched_foods) < 5:\n",
    "        print(f\"\\n‚ùå Error: Only found {len(matched_foods)} foods. Need at least 5.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Matched {len(matched_foods)} foods from your inventory\")\n",
    "    \n",
    "    # Step 2: Fetch nutrition data\n",
    "    print(f\"\\nüìä Step 2: Fetching nutritional data...\")\n",
    "    \n",
    "    food_names_escaped = [\"'\" + name.replace(\"'\", \"''\") + \"'\" for name in matched_foods]\n",
    "    food_names_str = ','.join(food_names_escaped)\n",
    "    \n",
    "    nutrition_query = f\"\"\"\n",
    "    SELECT \n",
    "        FOOD_NAME,\n",
    "        CATEGORY,\n",
    "        COALESCE(CALORIES, 0) as CALORIES,\n",
    "        COALESCE(PROTEIN, 0) as PROTEIN,\n",
    "        COALESCE(CARBOHYDRATE, 0) as CARBS,\n",
    "        COALESCE(TOTAL_FAT, 0) as FAT,\n",
    "        COALESCE(FIBER, 0) as FIBER\n",
    "    FROM {source_table}\n",
    "    WHERE FOOD_NAME IN ({food_names_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        nutrition_data = session.sql(nutrition_query).to_pandas()\n",
    "        print(f\"‚úÖ Retrieved nutrition data for {len(nutrition_data)} foods\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Build context\n",
    "    food_context = []\n",
    "    for _, row in nutrition_data.iterrows():\n",
    "        food_context.append(\n",
    "            f\"- {row['FOOD_NAME']} (Category: {row['CATEGORY']}): \"\n",
    "            f\"{row['CALORIES']} cal, {row['PROTEIN']}g protein, \"\n",
    "            f\"{row['CARBS']}g carbs, {row['FAT']}g fat per 100g\"\n",
    "        )\n",
    "    \n",
    "    context = \"\\n\".join(food_context)\n",
    "    \n",
    "    # Step 4: Build habits section for prompt\n",
    "    habits_section = \"\"\n",
    "    if daily_habits:\n",
    "        habits_section = \"\\n\\nUSER'S DAILY HABITS (MUST INCLUDE):\\n\"\n",
    "        for habit_key, habit_value in daily_habits.items():\n",
    "            if habit_key == \"avoid\":\n",
    "                habits_section += f\"- Foods to AVOID: {', '.join(habit_value) if isinstance(habit_value, list) else habit_value}\\n\"\n",
    "            elif habit_key == \"breakfast\":\n",
    "                habits_section += f\"- EVERY morning breakfast: {habit_value} (MANDATORY - include this every day)\\n\"\n",
    "            elif habit_key == \"post_workout\":\n",
    "                habits_section += f\"- Post-workout habit: {habit_value}\\n\"\n",
    "            elif habit_key == \"snack\":\n",
    "                habits_section += f\"- Preferred snack: {habit_value}\\n\"\n",
    "            else:\n",
    "                habits_section += f\"- {habit_key.replace('_', ' ').title()}: {habit_value}\\n\"\n",
    "    \n",
    "    # Step 5: Generate meal plan with habits\n",
    "    print(\"\\nü§ñ Step 3: Generating personalized meal plan with habits...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a professional chef and nutritionist creating a {days}-day meal plan with complete recipes.\n",
    "\n",
    "AVAILABLE INGREDIENTS (From User's Kitchen):\n",
    "{context}\n",
    "{habits_section}\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Create {days}-day meal plan with COMPLETE RECIPES\n",
    "- Daily calorie limit: {calorie_limit} calories\n",
    "- Daily carb limit: {carb_limit} grams\n",
    "{f'- Daily protein target: {protein_target} grams' if protein_target else ''}\n",
    "{f'- Dietary preference: {dietary_preferences}' if dietary_preferences else ''}\n",
    "- Include: Breakfast, Lunch, Snacks, Dinner for each day\n",
    "- ONLY use ingredients from the available list above\n",
    "- **STRICTLY FOLLOW user's daily habits (marked MANDATORY must be included every single day)**\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "\n",
    "**Day [X]:**\n",
    "\n",
    "---\n",
    "\n",
    "### Breakfast: [Recipe Name]\n",
    "**Prep Time:** [X] minutes | **Cook Time:** [X] minutes\n",
    "{\"**[‚≠ê DAILY HABIT]**\" if daily_habits and \"breakfast\" in daily_habits else \"\"}\n",
    "\n",
    "**Ingredients:**\n",
    "- [Ingredient 1] ([amount])\n",
    "- [Ingredient 2] ([amount])\n",
    "\n",
    "**Cooking Instructions:**\n",
    "1. [Detailed step with technique]\n",
    "2. [Detailed step with time/temp]\n",
    "3. [Final step]\n",
    "\n",
    "**Nutrition (per serving):**\n",
    "Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "---\n",
    "\n",
    "### Lunch: [Recipe Name]\n",
    "**Prep Time:** [X] minutes | **Cook Time:** [X] minutes\n",
    "\n",
    "**Ingredients:**\n",
    "- [Ingredient 1] ([amount])\n",
    "- [Ingredient 2] ([amount])\n",
    "\n",
    "**Cooking Instructions:**\n",
    "1. [Detailed step]\n",
    "2. [Detailed step]\n",
    "\n",
    "**Nutrition (per serving):**\n",
    "Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "---\n",
    "\n",
    "### Snacks: [Recipe Name]\n",
    "**Prep Time:** [X] minutes\n",
    "\n",
    "**Ingredients:**\n",
    "- [Ingredient 1] ([amount])\n",
    "\n",
    "**Instructions:**\n",
    "1. [Simple preparation]\n",
    "\n",
    "**Nutrition:**\n",
    "Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "---\n",
    "\n",
    "### Dinner: [Recipe Name]\n",
    "**Prep Time:** [X] minutes | **Cook Time:** [X] minutes\n",
    "\n",
    "**Ingredients:**\n",
    "- [Ingredient 1] ([amount])\n",
    "- [Ingredient 2] ([amount])\n",
    "\n",
    "**Cooking Instructions:**\n",
    "1. [Detailed step]\n",
    "2. [Detailed step]\n",
    "\n",
    "**Nutrition (per serving):**\n",
    "Calories: [X] cal | Protein: [X]g | Carbs: [X]g | Fat: [X]g\n",
    "\n",
    "---\n",
    "\n",
    "**Day [X] Totals:**\n",
    "- Total Calories: [X] / {calorie_limit} cal\n",
    "- Total Protein: [X]g\n",
    "- Total Carbs: [X] / {carb_limit}g\n",
    "- Total Fat: [X]g\n",
    "- Status: ‚úì Within limits\n",
    "\n",
    "---\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. If user has a MANDATORY breakfast habit (like \"whey protein milkshake\"), include it EVERY single day as breakfast\n",
    "2. Provide complete recipe for habit items (e.g., for whey shake: list whey powder, milk, banana amounts)\n",
    "3. Calculate nutrition for habit items and subtract from daily totals when planning other meals\n",
    "4. Give creative names to each meal\n",
    "5. List realistic prep/cook times\n",
    "6. Provide step-by-step instructions with temperatures and techniques\n",
    "7. Use exact measurements (cups, grams, tablespoons)\n",
    "8. Scale nutrition based on portion sizes\n",
    "9. Account for user habits in calorie/macro distribution\n",
    "10. Make recipes simple and practical\n",
    "\n",
    "Generate the complete {days}-day meal plan with detailed recipes now:\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_query = f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "            'mistral-large2',\n",
    "            '{prompt.replace(\"'\", \"''\")}'\n",
    "        ) as MEAL_PLAN\n",
    "        \"\"\"\n",
    "        \n",
    "        response = session.sql(llm_query).collect()[0]['MEAL_PLAN']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"üìã YOUR PERSONALIZED {days}-DAY MEAL PLAN\")\n",
    "        print(\"=\"*70)\n",
    "        print(response)\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Display habits summary\n",
    "        if daily_habits:\n",
    "            print(f\"\\nüîÑ Daily Habits Applied:\")\n",
    "            print(\"-\"*70)\n",
    "            for habit_key, habit_value in daily_habits.items():\n",
    "                print(f\"  ‚Ä¢ {habit_key.replace('_', ' ').title()}: {habit_value}\")\n",
    "        \n",
    "        return {\n",
    "            'days': days,\n",
    "            'calorie_limit': calorie_limit,\n",
    "            'carb_limit': carb_limit,\n",
    "            'protein_target': protein_target,\n",
    "            'daily_habits': daily_habits,\n",
    "            'user_inventory': inventory,\n",
    "            'matched_foods': matched_foods,\n",
    "            'inventory_mapping': food_search_results,\n",
    "            'available_foods_count': len(nutrition_data),\n",
    "            'meal_plan_with_recipes': response,\n",
    "            'nutrition_data': nutrition_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Habit-aware meal plan generator ready!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"\"\"\n",
    "# Define your habits\n",
    "my_habits = {\n",
    "    \"breakfast\": \"whey protein milkshake with banana\",\n",
    "    \"snack\": \"greek yogurt with almonds\",\n",
    "    \"avoid\": [\"dairy\", \"gluten\"]  # optional\n",
    "}\n",
    "\n",
    "# Your inventory\n",
    "my_inventory = [\n",
    "    \"whey protein powder\", \"milk\", \"banana\", \"eggs\",\n",
    "    \"chicken breast\", \"salmon\", \"brown rice\", \"quinoa\",\n",
    "    \"spinach\", \"broccoli\", \"greek yogurt\", \"almonds\"\n",
    "]\n",
    "\n",
    "plan = rag_generate_meal_plan_with_habits(\n",
    "    inventory=my_inventory,\n",
    "    days=2,\n",
    "    calorie_limit=2000,\n",
    "    carb_limit=200,\n",
    "    protein_target=120,\n",
    "    dietary_preferences=\"balanced healthy\",\n",
    "    daily_habits=my_habits\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa55b6e-a53e-4d44-a580-0b910267baae",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "# Define your daily habits\n",
    "my_daily_habits = {\n",
    "    \"breakfast\": \"whey protein milkshake with banana and milk\",  # This will be EVERY morning\n",
    "    \"snack\": \"greek yogurt with berries\",\n",
    "    \"post_workout\": \"banana with peanut butter\"\n",
    "}\n",
    "\n",
    "# Your kitchen inventory\n",
    "my_inventory = [\n",
    "    # For your daily shake\n",
    "    \"whey protein powder\", \"milk\", \"banana\", \"berries\",\n",
    "    \n",
    "    # Proteins\n",
    "    \"eggs\", \"chicken breast\", \"salmon\", \"ground turkey\",\n",
    "    \"greek yogurt\", \"cottage cheese\", \"tofu\",\n",
    "    \n",
    "    # Carbs\n",
    "    \"brown rice\", \"quinoa\", \"oatmeal\", \"sweet potato\",\n",
    "    \"whole wheat bread\", \"pasta\",\n",
    "    \n",
    "    # Vegetables\n",
    "    \"spinach\", \"broccoli\", \"carrots\", \"bell peppers\",\n",
    "    \"tomatoes\", \"zucchini\", \"onions\", \"garlic\",\n",
    "    \n",
    "    # Fats & Others\n",
    "    \"peanut butter\", \"almonds\", \"olive oil\", \"avocado\",\n",
    "    \"cheese\", \"butter\", \"honey\"\n",
    "]\n",
    "\n",
    "# Generate meal plan with your habits\n",
    "result = rag_generate_meal_plan_with_habits(\n",
    "    inventory=my_inventory,\n",
    "    days=2,\n",
    "    calorie_limit=2200,  # Higher since you have protein shake\n",
    "    carb_limit=220,\n",
    "    protein_target=150,  # Higher protein target for fitness\n",
    "    dietary_preferences=\"high protein balanced meals\",\n",
    "    daily_habits=my_daily_habits\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n‚úÖ Personalized meal plan created!\")\n",
    "    print(f\"üìä Using {result['available_foods_count']} ingredients\")\n",
    "    print(f\"üîÑ Daily habits incorporated: {len(result['daily_habits'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694c081-e743-44d9-bb88-95bc45339aca",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "# Complex habits configuration\n",
    "advanced_habits = {\n",
    "    \"breakfast\": \"whey protein milkshake with banana and oats (30g protein)\",\n",
    "    \"pre_workout\": \"black coffee with banana\",\n",
    "    \"post_workout\": \"protein shake with berries\",\n",
    "    \"evening_snack\": \"cottage cheese with almonds\",\n",
    "    \"avoid\": [\"dairy in dinner\", \"fried foods\", \"processed sugar\"],\n",
    "    \"timing\": \"early dinner by 7 PM\"\n",
    "}\n",
    "\n",
    "advanced_plan = rag_generate_meal_plan_with_habits(\n",
    "    inventory=my_inventory,\n",
    "    days=3,\n",
    "    calorie_limit=2400,\n",
    "    carb_limit=250,\n",
    "    protein_target=160,\n",
    "    dietary_preferences=\"bodybuilding meal prep\",\n",
    "    daily_habits=advanced_habits\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac69da-42f0-4b22-99bb-7c9b3a4f01d3",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä ADVANCED QUANTITATIVE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "# Initialize results dictionary\n",
    "quantitative_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfd139-6efc-4a5e-ac3f-f6c666d1ba7c",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": [
    "print(\"\\n1Ô∏è‚É£ EMBEDDING DISTRIBUTION ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Get sample embeddings for analysis (100 from each model)\n",
    "snowflake_sample = session.sql(f\"\"\"\n",
    "    SELECT EMBEDDING\n",
    "    FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS\n",
    "    LIMIT 100\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "openai_sample = session.sql(f\"\"\"\n",
    "    SELECT EMBEDDING\n",
    "    FROM {embeddings_schema}.OPENAI_EMBEDDINGS\n",
    "    WHERE EMBEDDING IS NOT NULL\n",
    "    LIMIT 100\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "if len(snowflake_sample) > 0:\n",
    "    # Convert embeddings to numpy arrays\n",
    "    snow_embeddings = np.array([eval(str(e)) if isinstance(e, str) else e for e in snowflake_sample['EMBEDDING'].values])\n",
    "    \n",
    "    # Calculate statistics for Snowflake\n",
    "    snow_means = np.mean(snow_embeddings, axis=0)\n",
    "    snow_stds = np.std(snow_embeddings, axis=0)\n",
    "    \n",
    "    print(\"üìç Snowflake Arctic Embedding Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Mean of means: {np.mean(snow_means):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Std of means: {np.std(snow_means):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Mean of stds: {np.mean(snow_stds):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Embedding norm (avg): {np.mean([np.linalg.norm(e) for e in snow_embeddings]):.4f}\")\n",
    "    \n",
    "    quantitative_metrics['snowflake_distribution'] = {\n",
    "        'mean_of_means': float(np.mean(snow_means)),\n",
    "        'std_of_means': float(np.std(snow_means)),\n",
    "        'mean_of_stds': float(np.mean(snow_stds)),\n",
    "        'avg_norm': float(np.mean([np.linalg.norm(e) for e in snow_embeddings]))\n",
    "    }\n",
    "\n",
    "if len(openai_sample) > 0:\n",
    "    # Similar for OpenAI\n",
    "    openai_embeddings = np.array([eval(str(e)) if isinstance(e, str) else e for e in openai_sample['EMBEDDING'].values])\n",
    "    \n",
    "    openai_means = np.mean(openai_embeddings, axis=0)\n",
    "    openai_stds = np.std(openai_embeddings, axis=0)\n",
    "    \n",
    "    print(\"\\nüìç OpenAI Embedding Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Mean of means: {np.mean(openai_means):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Std of means: {np.std(openai_means):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Mean of stds: {np.mean(openai_stds):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Embedding norm (avg): {np.mean([np.linalg.norm(e) for e in openai_embeddings]):.4f}\")\n",
    "    \n",
    "    quantitative_metrics['openai_distribution'] = {\n",
    "        'mean_of_means': float(np.mean(openai_means)),\n",
    "        'std_of_means': float(np.std(openai_means)),\n",
    "        'mean_of_stds': float(np.mean(openai_stds)),\n",
    "        'avg_norm': float(np.mean([np.linalg.norm(e) for e in openai_embeddings]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd46b5-edfe-4761-8e9e-7c2a1a02da5b",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": [
    "print(\"\\n2Ô∏è‚É£ RETRIEVAL QUALITY METRICS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Test queries for retrieval metrics\n",
    "test_queries_retrieval = [\n",
    "    \"high protein vegetarian\",\n",
    "    \"low calorie breakfast\", \n",
    "    \"healthy snacks\",\n",
    "    \"mediterranean diet\",\n",
    "    \"gluten free options\"\n",
    "]\n",
    "\n",
    "precision_at_k = {'snowflake': [], 'openai': []}\n",
    "recall_scores = {'snowflake': [], 'openai': []}\n",
    "\n",
    "for query in test_queries_retrieval:\n",
    "    # Get top 10 results from each model\n",
    "    snow_results = search_snowflake(query, top_k=10)\n",
    "    openai_results = search_openai(query, top_k=10)\n",
    "    \n",
    "    if not snow_results.empty and not openai_results.empty:\n",
    "        # Calculate overlap (as proxy for precision)\n",
    "        snow_foods = set(snow_results['FOOD_NAME'].tolist()[:5])\n",
    "        openai_foods = set(openai_results['FOOD_NAME'].tolist()[:5])\n",
    "        \n",
    "        # Precision@5 (overlap as relevance proxy)\n",
    "        overlap = len(snow_foods.intersection(openai_foods))\n",
    "        precision_at_k['snowflake'].append(overlap / 5)\n",
    "        precision_at_k['openai'].append(overlap / 5)\n",
    "        \n",
    "        # Check if results contain query terms (simple relevance check)\n",
    "        query_terms = query.lower().split()\n",
    "        \n",
    "        snow_relevant = sum(1 for food in snow_foods if any(term in food.lower() for term in query_terms))\n",
    "        openai_relevant = sum(1 for food in openai_foods if any(term in food.lower() for term in query_terms))\n",
    "        \n",
    "        recall_scores['snowflake'].append(snow_relevant / 5)\n",
    "        recall_scores['openai'].append(openai_relevant / 5)\n",
    "\n",
    "if precision_at_k['snowflake']:\n",
    "    print(\"üìç Precision@5 (Average):\")\n",
    "    print(f\"   ‚Ä¢ Snowflake: {np.mean(precision_at_k['snowflake']):.3f}\")\n",
    "    print(f\"   ‚Ä¢ OpenAI: {np.mean(precision_at_k['openai']):.3f}\")\n",
    "    \n",
    "    print(\"\\nüìç Relevance Score (Query term matching):\")\n",
    "    print(f\"   ‚Ä¢ Snowflake: {np.mean(recall_scores['snowflake']):.3f}\")\n",
    "    print(f\"   ‚Ä¢ OpenAI: {np.mean(recall_scores['openai']):.3f}\")\n",
    "    \n",
    "    quantitative_metrics['retrieval_quality'] = {\n",
    "        'snowflake_precision_at_5': float(np.mean(precision_at_k['snowflake'])),\n",
    "        'openai_precision_at_5': float(np.mean(precision_at_k['openai'])),\n",
    "        'snowflake_relevance': float(np.mean(recall_scores['snowflake'])),\n",
    "        'openai_relevance': float(np.mean(recall_scores['openai']))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14543687-44a0-4154-a0c5-df4a18e1f1c6",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
    "print(\"\\n3Ô∏è‚É£ SEMANTIC COHERENCE ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Get embeddings for specific food categories\n",
    "categories_to_test = ['Vegetables', 'Fruits', 'Dairy', 'Grains']\n",
    "intra_category_similarity = {'snowflake': [], 'openai': []}\n",
    "inter_category_similarity = {'snowflake': [], 'openai': []}\n",
    "\n",
    "for category in categories_to_test[:2]:  # Test first 2 categories for speed\n",
    "    # Get embeddings for this category from Snowflake\n",
    "    snow_category = session.sql(f\"\"\"\n",
    "        SELECT EMBEDDING\n",
    "        FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS\n",
    "        WHERE CATEGORY = '{category}'\n",
    "        LIMIT 20\n",
    "    \"\"\").to_pandas()\n",
    "    \n",
    "    if len(snow_category) >= 10:\n",
    "        embeddings = np.array([eval(str(e)) if isinstance(e, str) else e for e in snow_category['EMBEDDING'].values[:10]])\n",
    "        \n",
    "        # Calculate intra-category similarity (should be high for good embeddings)\n",
    "        similarities = []\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                cos_sim = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "                similarities.append(cos_sim)\n",
    "        \n",
    "        if similarities:\n",
    "            intra_category_similarity['snowflake'].append(np.mean(similarities))\n",
    "    \n",
    "    # Same for OpenAI\n",
    "    openai_category = session.sql(f\"\"\"\n",
    "        SELECT EMBEDDING\n",
    "        FROM {embeddings_schema}.OPENAI_EMBEDDINGS\n",
    "        WHERE CATEGORY = '{category}'\n",
    "        LIMIT 20\n",
    "    \"\"\").to_pandas()\n",
    "    \n",
    "    if len(openai_category) >= 10:\n",
    "        embeddings = np.array([eval(str(e)) if isinstance(e, str) else e for e in openai_category['EMBEDDING'].values[:10]])\n",
    "        \n",
    "        similarities = []\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                cos_sim = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "                similarities.append(cos_sim)\n",
    "        \n",
    "        if similarities:\n",
    "            intra_category_similarity['openai'].append(np.mean(similarities))\n",
    "\n",
    "if intra_category_similarity['snowflake']:\n",
    "    print(\"üìç Intra-Category Similarity (Higher is better):\")\n",
    "    print(f\"   ‚Ä¢ Snowflake: {np.mean(intra_category_similarity['snowflake']):.4f}\")\n",
    "    if intra_category_similarity['openai']:\n",
    "        print(f\"   ‚Ä¢ OpenAI: {np.mean(intra_category_similarity['openai']):.4f}\")\n",
    "    \n",
    "    quantitative_metrics['semantic_coherence'] = {\n",
    "        'snowflake_intra_similarity': float(np.mean(intra_category_similarity['snowflake'])),\n",
    "        'openai_intra_similarity': float(np.mean(intra_category_similarity['openai'])) if intra_category_similarity['openai'] else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd039bac-7bb8-49ee-9933-ae48a2c05f36",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": [
    "print(\"\\n4Ô∏è‚É£ RESPONSE TIME & THROUGHPUT ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Measure search response times\n",
    "search_times_snow = []\n",
    "search_times_openai = []\n",
    "test_search_queries = [\"protein\", \"low calorie\", \"vegetarian\", \"breakfast\", \"snack\"]\n",
    "\n",
    "for q in test_search_queries:\n",
    "    # Snowflake search time\n",
    "    start = time.time()\n",
    "    _ = search_snowflake(q, top_k=10)\n",
    "    search_times_snow.append((time.time() - start) * 1000)\n",
    "    \n",
    "    # OpenAI search time\n",
    "    if client:\n",
    "        start = time.time()\n",
    "        _ = search_openai(q, top_k=10)\n",
    "        search_times_openai.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"üìç Search Response Times (ms):\")\n",
    "print(f\"   ‚Ä¢ Snowflake:\")\n",
    "print(f\"     - Mean: {np.mean(search_times_snow):.2f}ms\")\n",
    "print(f\"     - Median: {np.median(search_times_snow):.2f}ms\")\n",
    "print(f\"     - P95: {np.percentile(search_times_snow, 95):.2f}ms\")\n",
    "\n",
    "if search_times_openai:\n",
    "    print(f\"   ‚Ä¢ OpenAI:\")\n",
    "    print(f\"     - Mean: {np.mean(search_times_openai):.2f}ms\")\n",
    "    print(f\"     - Median: {np.median(search_times_openai):.2f}ms\")\n",
    "    print(f\"     - P95: {np.percentile(search_times_openai, 95):.2f}ms\")\n",
    "\n",
    "# Calculate throughput\n",
    "snow_throughput = 1000 / np.mean(search_times_snow) if search_times_snow else 0  # queries per second\n",
    "openai_throughput = 1000 / np.mean(search_times_openai) if search_times_openai else 0\n",
    "\n",
    "print(f\"\\nüìç Theoretical Throughput:\")\n",
    "print(f\"   ‚Ä¢ Snowflake: {snow_throughput:.1f} queries/second\")\n",
    "if search_times_openai:\n",
    "    print(f\"   ‚Ä¢ OpenAI: {openai_throughput:.1f} queries/second\")\n",
    "\n",
    "quantitative_metrics['performance'] = {\n",
    "    'snowflake_mean_search_ms': float(np.mean(search_times_snow)),\n",
    "    'snowflake_p95_search_ms': float(np.percentile(search_times_snow, 95)),\n",
    "    'snowflake_throughput_qps': float(snow_throughput),\n",
    "    'openai_mean_search_ms': float(np.mean(search_times_openai)) if search_times_openai else 0,\n",
    "    'openai_p95_search_ms': float(np.percentile(search_times_openai, 95)) if search_times_openai else 0,\n",
    "    'openai_throughput_qps': float(openai_throughput)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc8b5c-46e9-4854-a6fe-294cf5de292d",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": [
    "print(\"\\n6Ô∏è‚É£ DIVERSITY & COVERAGE METRICS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Analyze unique tokens/concepts covered\n",
    "unique_categories_snow = session.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT CATEGORY) as cnt \n",
    "    FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS \n",
    "    WHERE CATEGORY IS NOT NULL\n",
    "\"\"\").collect()[0]['CNT']\n",
    "\n",
    "unique_categories_openai = session.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT CATEGORY) as cnt \n",
    "    FROM {embeddings_schema}.OPENAI_EMBEDDINGS \n",
    "    WHERE CATEGORY IS NOT NULL\n",
    "\"\"\").collect()[0]['CNT']\n",
    "\n",
    "# Get diversity in search results\n",
    "diversity_queries = [\"food\", \"healthy\", \"meal\"]\n",
    "diversity_scores = {'snowflake': [], 'openai': []}\n",
    "\n",
    "for query in diversity_queries:\n",
    "    snow_results = search_snowflake(query, top_k=20)\n",
    "    if not snow_results.empty:\n",
    "        unique_categories_in_results = len(snow_results['CATEGORY'].dropna().unique())\n",
    "        diversity_scores['snowflake'].append(unique_categories_in_results / 20)\n",
    "    \n",
    "    openai_results = search_openai(query, top_k=20)\n",
    "    if not openai_results.empty:\n",
    "        unique_categories_in_results = len(openai_results['CATEGORY'].dropna().unique())\n",
    "        diversity_scores['openai'].append(unique_categories_in_results / 20)\n",
    "\n",
    "print(\"üìç Coverage Metrics:\")\n",
    "print(f\"   ‚Ä¢ Unique categories indexed:\")\n",
    "print(f\"     - Snowflake: {unique_categories_snow}\")\n",
    "print(f\"     - OpenAI: {unique_categories_openai}\")\n",
    "\n",
    "if diversity_scores['snowflake']:\n",
    "    print(\"\\nüìç Result Diversity (avg unique categories in top-20):\")\n",
    "    print(f\"   ‚Ä¢ Snowflake: {np.mean(diversity_scores['snowflake']):.3f}\")\n",
    "    if diversity_scores['openai']:\n",
    "        print(f\"   ‚Ä¢ OpenAI: {np.mean(diversity_scores['openai']):.3f}\")\n",
    "\n",
    "quantitative_metrics['diversity'] = {\n",
    "    'snowflake_categories_covered': int(unique_categories_snow),\n",
    "    'openai_categories_covered': int(unique_categories_openai),\n",
    "    'snowflake_result_diversity': float(np.mean(diversity_scores['snowflake'])) if diversity_scores['snowflake'] else 0,\n",
    "    'openai_result_diversity': float(np.mean(diversity_scores['openai'])) if diversity_scores['openai'] else 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7832a-73cd-4715-a82d-cd10c755e2f6",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": [
    "print(\"\\n7Ô∏è‚É£ STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Compare search times for statistical significance\n",
    "if len(search_times_snow) > 1 and len(search_times_openai) > 1:\n",
    "    t_stat, p_value = stats.ttest_ind(search_times_snow, search_times_openai)\n",
    "    \n",
    "    print(\"üìç Response Time Comparison (t-test):\")\n",
    "    print(f\"   ‚Ä¢ t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"   ‚Ä¢ p-value: {p_value:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Significant difference: {'Yes' if p_value < 0.05 else 'No'} (Œ±=0.05)\")\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((np.var(search_times_snow) + np.var(search_times_openai)) / 2)\n",
    "    cohens_d = (np.mean(search_times_snow) - np.mean(search_times_openai)) / pooled_std\n",
    "    \n",
    "    print(f\"\\nüìç Effect Size (Cohen's d): {abs(cohens_d):.3f}\")\n",
    "    print(f\"   ‚Ä¢ Interpretation: \", end=\"\")\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        print(\"Negligible\")\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        print(\"Small\")\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        print(\"Medium\")\n",
    "    else:\n",
    "        print(\"Large\")\n",
    "    \n",
    "    quantitative_metrics['statistical_tests'] = {\n",
    "        't_statistic': float(t_stat),\n",
    "        'p_value': float(p_value),\n",
    "        'cohens_d': float(cohens_d),\n",
    "        'significant_difference': p_value < 0.05\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb477c-8c9e-4ea0-b1fb-3bd7b32d1715",
   "metadata": {
    "collapsed": false,
    "name": "cell39"
   },
   "source": [
    "LLM-Based Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed0286-d53f-412a-9fc6-b6bd6da967d3",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ü§ñ LLM-BASED QUANTITATIVE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Initialize LLM metrics storage\n",
    "llm_metrics = {\n",
    "    'rag_quality': {},\n",
    "    'response_coherence': {},\n",
    "    'factual_accuracy': {},\n",
    "    'context_relevance': {},\n",
    "    'generation_metrics': {}\n",
    "}\n",
    "\n",
    "# Define evaluation queries for different meal planning scenarios\n",
    "evaluation_queries = [\n",
    "    {\n",
    "        'query': 'Create a high-protein vegetarian breakfast menu',\n",
    "        'expected_keywords': ['protein', 'vegetarian', 'breakfast', 'eggs', 'dairy', 'legumes'],\n",
    "        'nutrition_focus': 'protein'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Suggest low-calorie snacks under 200 calories',\n",
    "        'expected_keywords': ['calories', 'snack', 'low', 'light', 'healthy'],\n",
    "        'nutrition_focus': 'calories'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Design a Mediterranean diet lunch plan',\n",
    "        'expected_keywords': ['mediterranean', 'olive', 'fish', 'vegetables', 'whole grains'],\n",
    "        'nutrition_focus': 'balanced'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Recommend foods for muscle recovery after workout',\n",
    "        'expected_keywords': ['protein', 'recovery', 'muscle', 'carbs', 'amino'],\n",
    "        'nutrition_focus': 'recovery'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Find gluten-free dinner options',\n",
    "        'expected_keywords': ['gluten-free', 'rice', 'quinoa', 'vegetables', 'meat'],\n",
    "        'nutrition_focus': 'dietary_restriction'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìã Prepared {len(evaluation_queries)} evaluation queries\")\n",
    "print(\"   Categories: Protein focus, Calorie control, Diet styles, Recovery, Restrictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5aca1-dc91-40b7-a036-eeb15c904cd5",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "print(\"\\n1Ô∏è‚É£ RAG QUALITY EVALUATION - SNOWFLAKE ARCTIC\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "snowflake_rag_scores = {\n",
    "    'relevance_scores': [],\n",
    "    'response_times': [],\n",
    "    'context_quality': [],\n",
    "    'keyword_coverage': [],\n",
    "    'factual_accuracy': []\n",
    "}\n",
    "\n",
    "for eval_item in evaluation_queries[:3]:  # Test first 3 queries for demo\n",
    "    query = eval_item['query']\n",
    "    expected_keywords = eval_item['expected_keywords']\n",
    "    \n",
    "    print(f\"\\nüìù Query: '{query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Retrieve relevant foods using Snowflake embeddings\n",
    "    try:\n",
    "        results = session.sql(f\"\"\"\n",
    "            WITH query_embed AS (\n",
    "                SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "                    'snowflake-arctic-embed-m', \n",
    "                    '{query.replace(\"'\", \"''\")}' \n",
    "                ) as QUERY_EMBEDDING\n",
    "            )\n",
    "            SELECT \n",
    "                s.FDC_ID,\n",
    "                s.FOOD_NAME,\n",
    "                s.CATEGORY,\n",
    "                m.CALORIES,\n",
    "                m.PROTEIN,\n",
    "                m.CARBOHYDRATE,\n",
    "                m.TOTAL_FAT,\n",
    "                VECTOR_COSINE_SIMILARITY(\n",
    "                    s.EMBEDDING, \n",
    "                    q.QUERY_EMBEDDING\n",
    "                ) as SIMILARITY\n",
    "            FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS s\n",
    "            JOIN {source_table} m ON s.FDC_ID = m.FDC_ID,\n",
    "                 query_embed q\n",
    "            ORDER BY SIMILARITY DESC\n",
    "            LIMIT 5\n",
    "        \"\"\").to_pandas()\n",
    "        \n",
    "        retrieval_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Step 2: Prepare context for LLM\n",
    "        context_items = []\n",
    "        for _, row in results.iterrows():\n",
    "            context_items.append(\n",
    "                f\"- {row['FOOD_NAME']} ({row['CATEGORY']}): \"\n",
    "                f\"Calories: {row['CALORIES']:.0f}, \"\n",
    "                f\"Protein: {row['PROTEIN']:.1f}g, \"\n",
    "                f\"Carbs: {row['CARBOHYDRATE']:.1f}g, \"\n",
    "                f\"Fat: {row['TOTAL_FAT']:.1f}g\"\n",
    "            )\n",
    "        context = \"\\n\".join(context_items)\n",
    "        \n",
    "        # Step 3: Generate response with LLM\n",
    "        llm_prompt = f\"\"\"\n",
    "        Based on the following foods retrieved from our database:\n",
    "        {context}\n",
    "        \n",
    "        User Query: {query}\n",
    "        \n",
    "        Provide a detailed meal recommendation that addresses the query.\n",
    "        Include specific foods from the list and explain why they fit the requirements.\n",
    "        \"\"\"\n",
    "        \n",
    "        llm_start = time.time()\n",
    "        llm_response = session.sql(f\"\"\"\n",
    "            SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                'mistral-7b',\n",
    "                '{llm_prompt.replace(\"'\", \"''\")}'\n",
    "            ) as RESPONSE\n",
    "        \"\"\").collect()[0]['RESPONSE']\n",
    "        llm_time = (time.time() - llm_start) * 1000\n",
    "        \n",
    "        # Step 4: Evaluate response quality\n",
    "        # Check keyword coverage\n",
    "        response_lower = llm_response.lower()\n",
    "        keywords_found = sum(1 for keyword in expected_keywords if keyword in response_lower)\n",
    "        keyword_coverage = keywords_found / len(expected_keywords)\n",
    "        \n",
    "        # Check if retrieved foods are mentioned in response\n",
    "        foods_mentioned = sum(1 for _, row in results.iterrows() \n",
    "                             if row['FOOD_NAME'].lower() in response_lower)\n",
    "        context_usage = foods_mentioned / len(results) if len(results) > 0 else 0\n",
    "        \n",
    "        # Calculate average similarity score (relevance)\n",
    "        avg_similarity = results['SIMILARITY'].mean()\n",
    "        \n",
    "        # Store metrics\n",
    "        snowflake_rag_scores['relevance_scores'].append(avg_similarity)\n",
    "        snowflake_rag_scores['response_times'].append(retrieval_time + llm_time)\n",
    "        snowflake_rag_scores['context_quality'].append(context_usage)\n",
    "        snowflake_rag_scores['keyword_coverage'].append(keyword_coverage)\n",
    "        \n",
    "        print(f\"   ‚úÖ Relevance: {avg_similarity:.4f}\")\n",
    "        print(f\"   ‚úÖ Keyword coverage: {keyword_coverage:.2%}\")\n",
    "        print(f\"   ‚úÖ Context usage: {context_usage:.2%}\")\n",
    "        print(f\"   ‚úÖ Total time: {retrieval_time + llm_time:.2f}ms\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error: {str(e)}\")\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "if snowflake_rag_scores['relevance_scores']:\n",
    "    llm_metrics['snowflake_rag'] = {\n",
    "        'avg_relevance': float(np.mean(snowflake_rag_scores['relevance_scores'])),\n",
    "        'avg_response_time_ms': float(np.mean(snowflake_rag_scores['response_times'])),\n",
    "        'avg_context_usage': float(np.mean(snowflake_rag_scores['context_quality'])),\n",
    "        'avg_keyword_coverage': float(np.mean(snowflake_rag_scores['keyword_coverage']))\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Snowflake + LLM Aggregate Scores:\")\n",
    "    print(f\"   ‚Ä¢ Average Relevance: {llm_metrics['snowflake_rag']['avg_relevance']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Keyword Coverage: {llm_metrics['snowflake_rag']['avg_keyword_coverage']:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Context Usage: {llm_metrics['snowflake_rag']['avg_context_usage']:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Avg Response Time: {llm_metrics['snowflake_rag']['avg_response_time_ms']:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8cf58-86ef-4282-8f05-2e111a29551e",
   "metadata": {
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": [
    "print(\"\\n2Ô∏è‚É£ RAG QUALITY EVALUATION - OPENAI\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "openai_rag_scores = {\n",
    "    'relevance_scores': [],\n",
    "    'response_times': [],\n",
    "    'context_quality': [],\n",
    "    'keyword_coverage': [],\n",
    "    'factual_accuracy': []\n",
    "}\n",
    "\n",
    "if client is not None:\n",
    "    for eval_item in evaluation_queries[:3]:  # Test first 3 queries\n",
    "        query = eval_item['query']\n",
    "        expected_keywords = eval_item['expected_keywords']\n",
    "        \n",
    "        print(f\"\\nüìù Query: '{query}'\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Get query embedding from OpenAI\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=query\n",
    "            )\n",
    "            query_embedding = response.data[0].embedding\n",
    "            embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'\n",
    "            \n",
    "            # Retrieve relevant foods\n",
    "            results = session.sql(f\"\"\"\n",
    "                SELECT \n",
    "                    o.FDC_ID,\n",
    "                    o.FOOD_NAME,\n",
    "                    o.CATEGORY,\n",
    "                    m.CALORIES,\n",
    "                    m.PROTEIN,\n",
    "                    m.CARBOHYDRATE,\n",
    "                    m.TOTAL_FAT,\n",
    "                    VECTOR_COSINE_SIMILARITY(\n",
    "                        o.EMBEDDING,\n",
    "                        {embedding_str}::VECTOR(FLOAT, 1536)\n",
    "                    ) as SIMILARITY\n",
    "                FROM {embeddings_schema}.OPENAI_EMBEDDINGS o\n",
    "                JOIN {source_table} m ON o.FDC_ID = m.FDC_ID\n",
    "                ORDER BY SIMILARITY DESC\n",
    "                LIMIT 5\n",
    "            \"\"\").to_pandas()\n",
    "            \n",
    "            retrieval_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # Step 2: Prepare context for LLM\n",
    "            context_items = []\n",
    "            for _, row in results.iterrows():\n",
    "                context_items.append(\n",
    "                    f\"- {row['FOOD_NAME']} ({row['CATEGORY']}): \"\n",
    "                    f\"Calories: {row['CALORIES']:.0f}, \"\n",
    "                    f\"Protein: {row['PROTEIN']:.1f}g, \"\n",
    "                    f\"Carbs: {row['CARBOHYDRATE']:.1f}g, \"\n",
    "                    f\"Fat: {row['TOTAL_FAT']:.1f}g\"\n",
    "                )\n",
    "            context = \"\\n\".join(context_items)\n",
    "            \n",
    "            # Step 3: Generate response with LLM\n",
    "            llm_prompt = f\"\"\"\n",
    "            Based on the following foods retrieved from our database:\n",
    "            {context}\n",
    "            \n",
    "            User Query: {query}\n",
    "            \n",
    "            Provide a detailed meal recommendation that addresses the query.\n",
    "            Include specific foods from the list and explain why they fit the requirements.\n",
    "            \"\"\"\n",
    "            \n",
    "            llm_start = time.time()\n",
    "            llm_response = session.sql(f\"\"\"\n",
    "                SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                    'mistral-7b',\n",
    "                    '{llm_prompt.replace(\"'\", \"''\")}'\n",
    "                ) as RESPONSE\n",
    "            \"\"\").collect()[0]['RESPONSE']\n",
    "            llm_time = (time.time() - llm_start) * 1000\n",
    "            \n",
    "            # Step 4: Evaluate response quality\n",
    "            response_lower = llm_response.lower()\n",
    "            keywords_found = sum(1 for keyword in expected_keywords if keyword in response_lower)\n",
    "            keyword_coverage = keywords_found / len(expected_keywords)\n",
    "            \n",
    "            foods_mentioned = sum(1 for _, row in results.iterrows() \n",
    "                                 if row['FOOD_NAME'].lower() in response_lower)\n",
    "            context_usage = foods_mentioned / len(results) if len(results) > 0 else 0\n",
    "            \n",
    "            avg_similarity = results['SIMILARITY'].mean()\n",
    "            \n",
    "            # Store metrics\n",
    "            openai_rag_scores['relevance_scores'].append(avg_similarity)\n",
    "            openai_rag_scores['response_times'].append(retrieval_time + llm_time)\n",
    "            openai_rag_scores['context_quality'].append(context_usage)\n",
    "            openai_rag_scores['keyword_coverage'].append(keyword_coverage)\n",
    "            \n",
    "            print(f\"   ‚úÖ Relevance: {avg_similarity:.4f}\")\n",
    "            print(f\"   ‚úÖ Keyword coverage: {keyword_coverage:.2%}\")\n",
    "            print(f\"   ‚úÖ Context usage: {context_usage:.2%}\")\n",
    "            print(f\"   ‚úÖ Total time: {retrieval_time + llm_time:.2f}ms\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error: {str(e)}\")\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    if openai_rag_scores['relevance_scores']:\n",
    "        llm_metrics['openai_rag'] = {\n",
    "            'avg_relevance': float(np.mean(openai_rag_scores['relevance_scores'])),\n",
    "            'avg_response_time_ms': float(np.mean(openai_rag_scores['response_times'])),\n",
    "            'avg_context_usage': float(np.mean(openai_rag_scores['context_quality'])),\n",
    "            'avg_keyword_coverage': float(np.mean(openai_rag_scores['keyword_coverage']))\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä OpenAI + LLM Aggregate Scores:\")\n",
    "        print(f\"   ‚Ä¢ Average Relevance: {llm_metrics['openai_rag']['avg_relevance']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Keyword Coverage: {llm_metrics['openai_rag']['avg_keyword_coverage']:.2%}\")\n",
    "        print(f\"   ‚Ä¢ Context Usage: {llm_metrics['openai_rag']['avg_context_usage']:.2%}\")\n",
    "        print(f\"   ‚Ä¢ Avg Response Time: {llm_metrics['openai_rag']['avg_response_time_ms']:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac2336-44cb-490b-afff-5fc9b670cc3c",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": [
    "print(\"\\n3Ô∏è‚É£ COMPARATIVE LLM RESPONSE QUALITY\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Test specific scenarios for quality comparison\n",
    "quality_test_queries = [\n",
    "    \"What should I eat for a high-protein breakfast?\",\n",
    "    \"Suggest healthy snacks for weight loss\",\n",
    "    \"Create a balanced meal plan for diabetes management\"\n",
    "]\n",
    "\n",
    "comparative_scores = {\n",
    "    'snowflake': {'coherence': [], 'completeness': [], 'accuracy': []},\n",
    "    'openai': {'coherence': [], 'completeness': [], 'accuracy': []}\n",
    "}\n",
    "\n",
    "for test_query in quality_test_queries[:2]:  # Test first 2 for demo\n",
    "    print(f\"\\nüìù Testing: '{test_query}'\")\n",
    "    \n",
    "    # Get responses from both pipelines\n",
    "    for model_name in ['snowflake', 'openai']:\n",
    "        if model_name == 'openai' and client is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n   {model_name.upper()} Pipeline:\")\n",
    "        \n",
    "        try:\n",
    "            # Retrieve context\n",
    "            if model_name == 'snowflake':\n",
    "                retrieve_query = f\"\"\"\n",
    "                    WITH query_embed AS (\n",
    "                        SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "                            'snowflake-arctic-embed-m', \n",
    "                            '{test_query.replace(\"'\", \"''\")}' \n",
    "                        ) as QUERY_EMBEDDING\n",
    "                    )\n",
    "                    SELECT \n",
    "                        s.FOOD_NAME,\n",
    "                        s.CATEGORY,\n",
    "                        VECTOR_COSINE_SIMILARITY(s.EMBEDDING, q.QUERY_EMBEDDING) as SIMILARITY\n",
    "                    FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS s, query_embed q\n",
    "                    ORDER BY SIMILARITY DESC\n",
    "                    LIMIT 5\n",
    "                \"\"\"\n",
    "            else:  # OpenAI\n",
    "                response = client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=test_query\n",
    "                )\n",
    "                query_embedding = response.data[0].embedding\n",
    "                embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'\n",
    "                \n",
    "                retrieve_query = f\"\"\"\n",
    "                    SELECT \n",
    "                        FOOD_NAME,\n",
    "                        CATEGORY,\n",
    "                        VECTOR_COSINE_SIMILARITY(\n",
    "                            EMBEDDING,\n",
    "                            {embedding_str}::VECTOR(FLOAT, 1536)\n",
    "                        ) as SIMILARITY\n",
    "                    FROM {embeddings_schema}.OPENAI_EMBEDDINGS\n",
    "                    ORDER BY SIMILARITY DESC\n",
    "                    LIMIT 5\n",
    "                \"\"\"\n",
    "            \n",
    "            results = session.sql(retrieve_query).to_pandas()\n",
    "            \n",
    "            # Create context\n",
    "            context = \"\\n\".join([f\"- {row['FOOD_NAME']}\" for _, row in results.iterrows()])\n",
    "            \n",
    "            # Generate LLM response\n",
    "            llm_response = session.sql(f\"\"\"\n",
    "                SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                    'mistral-7b',\n",
    "                    'Based on these foods: {context.replace(\"'\", \"''\")}\n",
    "                    \n",
    "                    Query: {test_query.replace(\"'\", \"''\")}\n",
    "                    \n",
    "                    Provide a helpful meal recommendation.'\n",
    "                ) as RESPONSE\n",
    "            \"\"\").collect()[0]['RESPONSE']\n",
    "            \n",
    "            # Evaluate response with another LLM call\n",
    "            eval_prompt = f\"\"\"\n",
    "            Evaluate the following response on a scale of 1-10 for:\n",
    "            1. Coherence (logical flow and clarity)\n",
    "            2. Completeness (addresses all aspects of query)\n",
    "            3. Accuracy (nutritionally sound advice)\n",
    "            \n",
    "            Query: {test_query}\n",
    "            Response: {llm_response[:500]}\n",
    "            \n",
    "            Provide scores in format: Coherence: X/10, Completeness: Y/10, Accuracy: Z/10\n",
    "            \"\"\"\n",
    "            \n",
    "            eval_response = session.sql(f\"\"\"\n",
    "                SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                    'mistral-7b',\n",
    "                    '{eval_prompt.replace(\"'\", \"''\")}'\n",
    "                ) as EVALUATION\n",
    "            \"\"\").collect()[0]['EVALUATION']\n",
    "            \n",
    "            # Parse scores (simple extraction)\n",
    "            import re\n",
    "            scores = re.findall(r'(\\d+)/10', eval_response)\n",
    "            if len(scores) >= 3:\n",
    "                comparative_scores[model_name]['coherence'].append(int(scores[0]))\n",
    "                comparative_scores[model_name]['completeness'].append(int(scores[1]))\n",
    "                comparative_scores[model_name]['accuracy'].append(int(scores[2]))\n",
    "                \n",
    "                print(f\"   ‚Ä¢ Coherence: {scores[0]}/10\")\n",
    "                print(f\"   ‚Ä¢ Completeness: {scores[1]}/10\")\n",
    "                print(f\"   ‚Ä¢ Accuracy: {scores[2]}/10\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error in evaluation: {str(e)}\")\n",
    "\n",
    "# Calculate averages\n",
    "for model in ['snowflake', 'openai']:\n",
    "    if comparative_scores[model]['coherence']:\n",
    "        avg_coherence = np.mean(comparative_scores[model]['coherence'])\n",
    "        avg_completeness = np.mean(comparative_scores[model]['completeness'])\n",
    "        avg_accuracy = np.mean(comparative_scores[model]['accuracy'])\n",
    "        \n",
    "        llm_metrics[f'{model}_quality'] = {\n",
    "            'coherence_score': float(avg_coherence),\n",
    "            'completeness_score': float(avg_completeness),\n",
    "            'accuracy_score': float(avg_accuracy),\n",
    "            'overall_quality': float((avg_coherence + avg_completeness + avg_accuracy) / 3)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd1783-0d6a-483a-b4d2-65bc95fe9e5a",
   "metadata": {
    "language": "python",
    "name": "cell43"
   },
   "outputs": [],
   "source": [
    "print(\"\\n4Ô∏è‚É£ HALLUCINATION & FACTUAL GROUNDING TEST\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "hallucination_test_queries = [\n",
    "    {\n",
    "        'query': 'What is the protein content of chicken breast?',\n",
    "        'verify_field': 'PROTEIN',\n",
    "        'food_name_pattern': '%chicken%breast%'\n",
    "    },\n",
    "    {\n",
    "        'query': 'How many calories are in an apple?',\n",
    "        'verify_field': 'CALORIES',\n",
    "        'food_name_pattern': '%apple%'\n",
    "    }\n",
    "]\n",
    "\n",
    "factual_accuracy_scores = {'snowflake': [], 'openai': []}\n",
    "\n",
    "for test_item in hallucination_test_queries:\n",
    "    print(f\"\\nüìù Factual Query: '{test_item['query']}'\")\n",
    "    \n",
    "    # Get ground truth from database\n",
    "    ground_truth = session.sql(f\"\"\"\n",
    "        SELECT \n",
    "            FOOD_NAME,\n",
    "            {test_item['verify_field']} as VALUE\n",
    "        FROM {source_table}\n",
    "        WHERE LOWER(FOOD_NAME) LIKE LOWER('{test_item['food_name_pattern']}')\n",
    "        LIMIT 5\n",
    "    \"\"\").to_pandas()\n",
    "    \n",
    "    if not ground_truth.empty:\n",
    "        true_values = ground_truth['VALUE'].dropna().tolist()\n",
    "        avg_true_value = np.mean(true_values)\n",
    "        \n",
    "        print(f\"   üìå Ground Truth: {avg_true_value:.2f} (avg from {len(true_values)} items)\")\n",
    "        \n",
    "        for model in ['snowflake', 'openai']:\n",
    "            if model == 'openai' and client is None:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Generate response using RAG\n",
    "                if model == 'snowflake':\n",
    "                    context_query = f\"\"\"\n",
    "                        WITH query_embed AS (\n",
    "                            SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "                                'snowflake-arctic-embed-m', \n",
    "                                '{test_item['query'].replace(\"'\", \"''\")}' \n",
    "                            ) as QUERY_EMBEDDING\n",
    "                        )\n",
    "                        SELECT \n",
    "                            s.FOOD_NAME,\n",
    "                            m.{test_item['verify_field']} as VALUE\n",
    "                        FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS s\n",
    "                        JOIN {source_table} m ON s.FDC_ID = m.FDC_ID,\n",
    "                             query_embed q\n",
    "                        ORDER BY VECTOR_COSINE_SIMILARITY(s.EMBEDDING, q.QUERY_EMBEDDING) DESC\n",
    "                        LIMIT 3\n",
    "                    \"\"\"\n",
    "                else:  # OpenAI\n",
    "                    response = client.embeddings.create(\n",
    "                        model=\"text-embedding-3-small\",\n",
    "                        input=test_item['query']\n",
    "                    )\n",
    "                    query_embedding = response.data[0].embedding\n",
    "                    embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'\n",
    "                    \n",
    "                    context_query = f\"\"\"\n",
    "                        SELECT \n",
    "                            o.FOOD_NAME,\n",
    "                            m.{test_item['verify_field']} as VALUE\n",
    "                        FROM {embeddings_schema}.OPENAI_EMBEDDINGS o\n",
    "                        JOIN {source_table} m ON o.FDC_ID = m.FDC_ID\n",
    "                        ORDER BY VECTOR_COSINE_SIMILARITY(\n",
    "                            o.EMBEDDING,\n",
    "                            {embedding_str}::VECTOR(FLOAT, 1536)\n",
    "                        ) DESC\n",
    "                        LIMIT 3\n",
    "                    \"\"\"\n",
    "                \n",
    "                retrieved_data = session.sql(context_query).to_pandas()\n",
    "                \n",
    "                # Create factual context\n",
    "                context = \"\\n\".join([\n",
    "                    f\"- {row['FOOD_NAME']}: {row['VALUE']:.2f}\" \n",
    "                    for _, row in retrieved_data.iterrows()\n",
    "                ])\n",
    "                \n",
    "                # Generate response\n",
    "                llm_response = session.sql(f\"\"\"\n",
    "                    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                        'mistral-7b',\n",
    "                        'Based on our database:\n",
    "                        {context.replace(\"'\", \"''\")}\n",
    "                        \n",
    "                        Question: {test_item['query'].replace(\"'\", \"''\")}\n",
    "                        \n",
    "                        Provide a specific numerical answer based on the data above.'\n",
    "                    ) as RESPONSE\n",
    "                \"\"\").collect()[0]['RESPONSE']\n",
    "                \n",
    "                # Extract number from response\n",
    "                numbers = re.findall(r'\\d+\\.?\\d*', llm_response)\n",
    "                if numbers:\n",
    "                    predicted_value = float(numbers[0])\n",
    "                    error_rate = abs(predicted_value - avg_true_value) / avg_true_value\n",
    "                    accuracy = max(0, 1 - error_rate)\n",
    "                    \n",
    "                    factual_accuracy_scores[model].append(accuracy)\n",
    "                    \n",
    "                    print(f\"   {model.upper()}: Predicted {predicted_value:.2f}, Accuracy: {accuracy:.2%}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error for {model}: {str(e)}\")\n",
    "\n",
    "# Store factual accuracy metrics\n",
    "for model in ['snowflake', 'openai']:\n",
    "    if factual_accuracy_scores[model]:\n",
    "        llm_metrics[f'{model}_factual'] = {\n",
    "            'avg_factual_accuracy': float(np.mean(factual_accuracy_scores[model])),\n",
    "            'min_accuracy': float(np.min(factual_accuracy_scores[model])),\n",
    "            'max_accuracy': float(np.max(factual_accuracy_scores[model]))\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7c61f-9448-40b1-94e6-95c320171cba",
   "metadata": {
    "language": "python",
    "name": "cell44"
   },
   "outputs": [],
   "source": [
    "print(\"\\n5Ô∏è‚É£ CONTEXT WINDOW UTILIZATION ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "context_sizes = [3, 5, 10, 20]\n",
    "context_performance = {'snowflake': {}, 'openai': {}}\n",
    "\n",
    "test_query = \"Create a comprehensive weekly meal plan for weight loss\"\n",
    "\n",
    "for context_size in context_sizes:\n",
    "    print(f\"\\nüìä Testing with context size: {context_size} items\")\n",
    "    \n",
    "    for model in ['snowflake', 'openai']:\n",
    "        if model == 'openai' and client is None:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Retrieve different amounts of context\n",
    "            if model == 'snowflake':\n",
    "                results = session.sql(f\"\"\"\n",
    "                    WITH query_embed AS (\n",
    "                        SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "                            'snowflake-arctic-embed-m', \n",
    "                            '{test_query.replace(\"'\", \"''\")}' \n",
    "                        ) as QUERY_EMBEDDING\n",
    "                    )\n",
    "                    SELECT \n",
    "                        s.FOOD_NAME,\n",
    "                        s.CATEGORY,\n",
    "                        VECTOR_COSINE_SIMILARITY(s.EMBEDDING, q.QUERY_EMBEDDING) as SIMILARITY\n",
    "                    FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS s, query_embed q\n",
    "                    ORDER BY SIMILARITY DESC\n",
    "                    LIMIT {context_size}\n",
    "                \"\"\").to_pandas()\n",
    "            else:  # OpenAI\n",
    "                response = client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=test_query\n",
    "                )\n",
    "                query_embedding = response.data[0].embedding\n",
    "                embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'\n",
    "                \n",
    "                results = session.sql(f\"\"\"\n",
    "                    SELECT \n",
    "                        FOOD_NAME,\n",
    "                        CATEGORY,\n",
    "                        VECTOR_COSINE_SIMILARITY(\n",
    "                            EMBEDDING,\n",
    "                            {embedding_str}::VECTOR(FLOAT, 1536)\n",
    "                        ) as SIMILARITY\n",
    "                    FROM {embeddings_schema}.OPENAI_EMBEDDINGS\n",
    "                    ORDER BY SIMILARITY DESC\n",
    "                    LIMIT {context_size}\n",
    "                \"\"\").to_pandas()\n",
    "            \n",
    "            # Measure response quality with different context sizes\n",
    "            avg_similarity = results['SIMILARITY'].mean()\n",
    "            diversity = len(results['CATEGORY'].dropna().unique()) / len(results)\n",
    "            \n",
    "            context_performance[model][context_size] = {\n",
    "                'avg_similarity': float(avg_similarity),\n",
    "                'category_diversity': float(diversity),\n",
    "                'items_retrieved': len(results)\n",
    "            }\n",
    "            \n",
    "            print(f\"   {model.upper()}: Similarity={avg_similarity:.4f}, Diversity={diversity:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error for {model}: {str(e)}\")\n",
    "\n",
    "# Determine optimal context size\n",
    "for model in ['snowflake', 'openai']:\n",
    "    if context_performance[model]:\n",
    "        # Find context size with best balance of relevance and diversity\n",
    "        scores = []\n",
    "        for size, metrics in context_performance[model].items():\n",
    "            score = metrics['avg_similarity'] * 0.7 + metrics['category_diversity'] * 0.3\n",
    "            scores.append((size, score))\n",
    "        \n",
    "        optimal_size = max(scores, key=lambda x: x[1])\n",
    "        llm_metrics[f'{model}_context_optimization'] = {\n",
    "            'optimal_context_size': optimal_size[0],\n",
    "            'optimal_score': float(optimal_size[1])\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   {model.upper()} Optimal Context Size: {optimal_size[0]} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2525a60-2701-4547-a098-e6389d4b3739",
   "metadata": {
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": [
    "print(\"\\n6Ô∏è‚É£ MULTI-TURN CONVERSATION COHERENCE\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "conversation_flow = [\n",
    "    \"I want to start eating healthier\",\n",
    "    \"What are some high-protein options?\",\n",
    "    \"Which of those are vegetarian?\",\n",
    "    \"Can you suggest a meal plan with those?\"\n",
    "]\n",
    "\n",
    "conversation_coherence = {'snowflake': [], 'openai': []}\n",
    "conversation_context = {'snowflake': [], 'openai': []}\n",
    "\n",
    "for model in ['snowflake', 'openai']:\n",
    "    if model == 'openai' and client is None:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{model.upper()} Multi-turn Test:\")\n",
    "    accumulated_context = []\n",
    "    \n",
    "    for turn_idx, query in enumerate(conversation_flow[:3]):  # Test first 3 turns\n",
    "        print(f\"\\n   Turn {turn_idx + 1}: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Build cumulative query\n",
    "            full_query = \" \".join(conversation_flow[:turn_idx + 1])\n",
    "            \n",
    "            # Retrieve relevant context\n",
    "            if model == 'snowflake':\n",
    "                results = session.sql(f\"\"\"\n",
    "                    WITH query_embed AS (\n",
    "                        SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "                            'snowflake-arctic-embed-m', \n",
    "                            '{full_query.replace(\"'\", \"''\")}' \n",
    "                        ) as QUERY_EMBEDDING\n",
    "                    )\n",
    "                    SELECT \n",
    "                        FOOD_NAME,\n",
    "                        CATEGORY,\n",
    "                        VECTOR_COSINE_SIMILARITY(EMBEDDING, QUERY_EMBEDDING) as SIMILARITY\n",
    "                    FROM {embeddings_schema}.SNOWFLAKE_EMBEDDINGS s, query_embed q\n",
    "                    ORDER BY SIMILARITY DESC\n",
    "                    LIMIT 3\n",
    "                \"\"\").to_pandas()\n",
    "            else:  # OpenAI\n",
    "                response = client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=full_query\n",
    "                )\n",
    "                query_embedding = response.data[0].embedding\n",
    "                embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'\n",
    "                \n",
    "                results = session.sql(f\"\"\"\n",
    "                    SELECT \n",
    "                        FOOD_NAME,\n",
    "                        CATEGORY,\n",
    "                        VECTOR_COSINE_SIMILARITY(\n",
    "                            EMBEDDING,\n",
    "                            {embedding_str}::VECTOR(FLOAT, 1536)\n",
    "                        ) as SIMILARITY\n",
    "                    FROM {embeddings_schema}.OPENAI_EMBEDDINGS\n",
    "                    ORDER BY SIMILARITY DESC\n",
    "                    LIMIT 3\n",
    "                \"\"\").to_pandas()\n",
    "            \n",
    "            # Check coherence: are results building on previous context?\n",
    "            current_foods = set(results['FOOD_NAME'].tolist())\n",
    "            \n",
    "            if accumulated_context:\n",
    "                # Check if maintaining context\n",
    "                overlap = len(current_foods.intersection(set(accumulated_context))) / len(current_foods)\n",
    "                conversation_coherence[model].append(overlap)\n",
    "                print(f\"   Context retention: {overlap:.2%}\")\n",
    "            \n",
    "            accumulated_context.extend(current_foods)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error: {str(e)}\")\n",
    "    \n",
    "    if conversation_coherence[model]:\n",
    "        llm_metrics[f'{model}_conversation'] = {\n",
    "            'avg_context_retention': float(np.mean(conversation_coherence[model])),\n",
    "            'conversation_coherence_score': float(1 - np.std(conversation_coherence[model]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65b955-8f88-4d04-96b6-75f6910ecd1d",
   "metadata": {
    "language": "python",
    "name": "cell46"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ü§ñ LLM-BASED EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüèÜ RAG PIPELINE PERFORMANCE:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if 'snowflake_rag' in llm_metrics and 'openai_rag' in llm_metrics:\n",
    "    print(\"üìä Snowflake Arctic + LLM:\")\n",
    "    for metric, value in llm_metrics['snowflake_rag'].items():\n",
    "        print(f\"   ‚Ä¢ {metric.replace('_', ' ').title()}: {value:.3f}\")\n",
    "    \n",
    "    print(\"\\nüìä OpenAI + LLM:\")\n",
    "    for metric, value in llm_metrics['openai_rag'].items():\n",
    "        print(f\"   ‚Ä¢ {metric.replace('_', ' ').title()}: {value:.3f}\")\n",
    "    \n",
    "    # Compare performance\n",
    "    snow_score = llm_metrics['snowflake_rag']['avg_relevance'] * llm_metrics['snowflake_rag']['avg_keyword_coverage']\n",
    "    openai_score = llm_metrics['openai_rag']['avg_relevance'] * llm_metrics['openai_rag']['avg_keyword_coverage']\n",
    "    \n",
    "    print(f\"\\nüéØ Overall RAG Quality Score:\")\n",
    "    print(f\"   ‚Ä¢ Snowflake: {snow_score:.4f}\")\n",
    "    print(f\"   ‚Ä¢ OpenAI: {openai_score:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Winner: {'Snowflake' if snow_score > openai_score else 'OpenAI'}\")\n",
    "\n",
    "print(\"\\nüìà RESPONSE QUALITY METRICS:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if 'snowflake_quality' in llm_metrics:\n",
    "    print(\"Snowflake Pipeline:\")\n",
    "    print(f\"   ‚Ä¢ Overall Quality: {llm_metrics['snowflake_quality']['overall_quality']:.1f}/10\")\n",
    "\n",
    "if 'openai_quality' in llm_metrics:\n",
    "    print(\"OpenAI Pipeline:\")\n",
    "    print(f\"   ‚Ä¢ Overall Quality: {llm_metrics['openai_quality']['overall_quality']:.1f}/10\")\n",
    "\n",
    "print(\"\\n‚úÖ FACTUAL ACCURACY:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if 'snowflake_factual' in llm_metrics:\n",
    "    print(f\"Snowflake: {llm_metrics['snowflake_factual']['avg_factual_accuracy']:.2%} accurate\")\n",
    "\n",
    "if 'openai_factual' in llm_metrics:\n",
    "    print(f\"OpenAI: {llm_metrics['openai_factual']['avg_factual_accuracy']:.2%} accurate\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "",
   "authorId": "6834909890749",
   "authorName": "GECKO",
   "lastEditTime": 1762704810415,
   "notebookId": "ex5l66eagt4dthtcn2k6",
   "sessionId": "2a5fe6df-a28d-469d-8913-ca05e572452a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
